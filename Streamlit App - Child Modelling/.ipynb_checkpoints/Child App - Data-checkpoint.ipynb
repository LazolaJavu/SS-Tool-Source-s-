{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515c166d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T19:21:58.420216Z",
     "start_time": "2023-08-02T19:12:15.342403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Guid': '05dbba57-6046-ea11-833a-00155d326100', 'FullName': 'Zandile\\xa0Mgube ', 'IdNumber': '8712180875082', 'PersonalNumber': '0799734179', 'BirthDate': '1987-12-18', 'CountryOfCitizenship': 'South African', 'Age': '34', 'InactivityComments': None, 'MonthsSinceFranchisee': 42, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-02-02T22:00:00Z', 'InactiveDate': None, 'Status': 'Active', 'Gender': 'Female', 'ProgrammeType': 'Full Week (Daymothers)', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': None, 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': '511c61ed-4fe6-ec11-8351-00155d326100'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'Guid': '46b0d437-5e46-ea11-833a-00155d326100', 'FullName': 'Nokwanda\\xa0Phungula ', 'IdNumber': '9110101294088', 'PersonalNumber': '0825839766', 'BirthDate': '1991-10-10', 'CountryOfCitizenship': 'South African', 'Age': '28', 'InactivityComments': 'Unable to reach', 'MonthsSinceFranchisee': 19, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-01-23T22:00:00Z', 'InactiveDate': '2021-08-01T22:00:00Z', 'Status': 'Inactive', 'Gender': 'Female', 'ProgrammeType': 'Playgroup', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': 'Franchisee is uncontactable', 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': 'b4e5c772-26ef-e911-8326-0800274bb0e4'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'FullName': 'Lerato\\xa0Lehutso ', 'DateOfBirth': '1986-06-06', 'AreaOfOperation': 'Greater Taung', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6289ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Active', 'Guid': '31cc9b33-0a57-ea11-833a-00155d326100'}\n",
      "{'FullName': 'Funeka \\xa0Khoza ', 'DateOfBirth': None, 'AreaOfOperation': None, 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6a89ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Inactive', 'Guid': 'ae18e686-ac07-eb11-8343-00155d326100'}\n",
      "[{\"Name\":\"KZN Branch\",\"Guid\":\"5a6b40ec-1e4f-eb11-8345-00155d326100\"},{\"Name\":\"KET\",\"Guid\":\"3ed99252-d216-ec11-834c-00155d326100\"},{\"Name\":\"Lesedi Solar\",\"Guid\":\"e7b9933d-da35-ed11-8351-00155d326100\"},{\"Name\":\"Letsatsi Solar\",\"Guid\":\"821d436d-df4a-ed11-8355-00155d326100\"},{\"Name\":\"PPTrust\",\"Guid\":\"90b04590-6c96-ed11-8356-00155d326100\"},{\"Name\":\"NW Branch\",\"Guid\":\"6289ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"GP Branch\",\"Guid\":\"6a89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Khululeka\",\"Guid\":\"6c89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"LETCEE\",\"Guid\":\"6e89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"TREE\",\"Guid\":\"7089ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Siyakholwa\",\"Guid\":\"4b16de13-e386-e611-80ca-0050568109d5\"},{\"Name\":\"SAYM\",\"Guid\":\"474b1eed-e486-e611-80ca-0050568109d5\"},{\"Name\":\"Diaconia\",\"Guid\":\"cb6f6d1a-e686-e611-80ca-0050568109d5\"},{\"Name\":\"LIMA\",\"Guid\":\"f3e19b66-e786-e611-80ca-0050568109d5\"},{\"Name\":\"ELRU\",\"Guid\":\"20b4261f-e886-e611-80ca-0050568109d5\"},{\"Name\":\"Lesedi Educare Association\",\"Guid\":\"7911a744-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"3L Development\",\"Guid\":\"812aa76c-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Molteno\",\"Guid\":\"9093d385-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Penreach\",\"Guid\":\"0804f6ac-4584-e811-817d-0800274bb0e4\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uMuziwabantu Local Municipality', 'Siyacuma', 'Lesedi Local Municipality', 'Ventersdorp Local Municipality', 'Ephraim Mogale', 'City of Tshwane Metropolitan', 'Phokwane Local', 'Rustenburg Local Municipality', 'Dr JF Moroka Municipality', 'Ekurhuleni Metropolitan Municipality', 'coj', 'Greater Giyani', 'Polokwane Local Municipality', 'Rand West Municipality', 'Bitou Local Municipality', 'Emfuleni Local Municipality', 'Lekwa-Teemane Local Municipality', 'Mfuleni Municipality', 'Johannesburg', 'Theewaterskloof Local Municipality', 'Greater Tzaneen Local Municipality', 'Emalahleni Local Municipality', 'Moretele Local Municipality', 'Elias Motswaledi', 'Lensburg Municipality', 'BCM', 'City of Matlosana Local Municipality', 'Alfred Duma Local Municipality', 'Okhahlamba Local Municipality', 'collins chabane', 'Ethekwini District municipality', 'Umvoti', 'Lesedi Municipality ', 'city of Johannesburg ', 'Dikgatlong', 'Breede Valley Local Municipality', 'Mafikeng Local Municipality', 'Raymond  Mhlaba', 'Nelson mandela Bay', 'Nkandla', 'Coj', 'Dr. Nkosazana Dlamini Zuma Municipality', 'Ekurhuleni', 'Mbombela Local Municipality', 'Kgetlengrivier Local Municipality', 'Umzumbe', 'Sol Plaatje Local Municipality', 'Umzumbe Local Municipality', 'Greater Phalaborwa', 'Sakhisizwe Local Municipality', 'Buffalo City Metropolitan Municipality', 'Umdoni Local Municipality', 'Raymond Mhlaba', 'King Sabata Dilindyebo', 'Kagisano-Molopo Local Municipality', 'City of Cape atown', 'Emfuleni', 'Drankenstein Municipality', 'Tshwane municipality', 'COE', 'Maphumulo Local Municipality', 'Ephraim Mogale Local Municipality', 'Tlokwe Local Municipality', 'Umsunduzi', 'Nkomazi Local Municipality', 'city of joburg ', 'Harry gwala district Municipality', 'City of Joburg ', 'uMshwathi Local Municipality', 'Dr Nkosazana Dlamini Zuma', 'Brade vally Municipality ', 'NDZ', 'Maquassi Hills Local Municipality', 'Amahlathi Local Municipality', 'Nelson Mandela', 'Hessequa Local Municipality', 'Madibeng Local Municipality', 'Sol Plaatje Local', 'Amathole District Municipality', 'Ditsobotla Local Municipality', 'Moses Kotane Local Municipality', 'Enoch Mgijima Local Municipality', 'Maruleng Local Municipality', 'Molemole Local Municipality', 'City of Johannesburg Metropolitan Municipality', 'Ditsobotla Local Municipality ', 'Nyandeni Local Mununicipality ', 'Buffalo City Metropolitan', 'Tshwane', 'Nelson Mandela Bay', 'Msinga Local Municipality', 'City of Ekurhuleni Metropolitan', '11', 'City of Ekurleni', 'tshwane municipality', 'Plettenberg Bay ', 'City of Ekurhuleni', 'Newcastle Local Municipality', 'Nyandeni Local Mununicipality', 'Mangaung Metropolitan Municipality', 'Dikgatlong Local Municipality', 'Greater Taung Local Municipality', 'Bushbuckridge Municipality', 'Drakenstein', 'Eden District Municipality', 'Stellenbosch Local Municipality', 'Umzimkhulu', 'Knysna Local Municipality', 'Moretele', 'Newcastle', 'Enoch Mgijima', 'Nyandeni Local Municipality', 'City of Johannesburg', 'Bushbuckridge Local Municipality', 'Thembisile Hani Local Municipality', 'Breede Vally Municipality ', 'Harry gwala municipality', 'eThekwini Municipality', 'uMlalazi Local Municipality', 'Tshwane Municipality', 'Tshwane ', 'Midvaal', 'Tsantsabane Local Municipality', 'Saldanha Bay', 'Greater Palaborwa', 'uMlalazi', 'merafong', 'Water Sisulu Local Municipality', 'Bcm', 'Merafong Municipality', 'uMhlathuze Local Municipality', 'Mandeni', 'Emfulani', 'Kou-Kamma Municipality', 'Intsika Yethu Local Municipality', 'Rand West', 'Cape Winelands District Municipality', 'Joe Slovo', 'Letsemeng Local Municipality', 'Greater Tzaneen', 'Musina local municipality', 'Setsoto', 'Harry gwala district municipality', 'Jozini Local Municipality', 'Merafong', 'Msunduzi Local Municipality', 'West Rand District Municipality', 'Mamvini', 'Joe Morolong', 'Lesedi Municipality', 'Johannesburg City', 'Raymond Mhlaba Local Municipality', '0000', 'City of Cape Town', 'Pletternberg Bay ', 'City Of Johannesburg', 'Ephraim Moghale', 'Breede Valley Local', 'uMsinga Municipality', 'City of Cape Town Metropolitan Municipality', 'Saldanha', 'Umhlathuze', 'Cederberg Local Municipality', 'Inkosi Langalibalele Local Municipality', 'Kuilsrivier Municipality', 'Nkomazi Muncipality', 'City OF Johannesburg', 'Emalahleni', 'Ehlanzeni District Municipality', 'Ngqushwa Local Municipality', 'Ray Nkonyeni Local Municipality', 'Dispacth', 'Vhembe District Municipality', 'Umzimkhulu Local Municipality', 'Msukaligwa Municipality', 'Msinga Municipality', 'Drain', 'Lesedi', 'Collins Chabane Local Municipality', 'tshwane', 'Masilonyana Local Municipality', 'Sedibeng District Municipality', 'Kgatelopele Local Municipality', 'Oudtshoorn Local Municipality', 'Nkandla Local Municipality', 'George Local Municipality', 'City of Tshwane Metropolitan Municipality', 'Bojanala', 'Greater Letaba Local Municipality', 'Harry Gwala District Municipality', 'King Sabata Dalindyebo Local', 'Tokologo Local Municipality', 'Ngqushwa Municipality', 'Magareng Local Municipality', 'City of Ekhurhuleni', 'city of johannesburg', 'King Sabata Dalindyebo Local Municipality', 'Umvoti Local Municipality', 'Drakenstein Local', 'Laingsburg Local Municipality', 'Mantsopa Local Municipality', 'Joe Morolong Local Municipality', 'Great Kei Local Municipality', 'Drakenstein Municipality', 'Ehlanzeni', 'Nelson Mandela Bay Metropolitan Municipality', 'Raymond   Mhlaba', 'Nelson Mandela bay', 'Eden district munisipaliteit', 'Greater Maruleng', 'Ubuhlebezwe Local Municipality', 'Amahlathi', 'Greater Tubatse Local Municipality', 'Ndwedwe Local Municipality', 'Johannesburg Municipality', 'uMfolozi Local Municipality', 'eThekwini', 'Greater Giyani Local Municipality', 'Naledi Local Municipality', 'Saldanha Bay Local Municipality', 'COJ', 'Tswelopele Local Municipality', 'Tokologo local municipality', 'Thembelihle Local Municipality', 'Swartland', 'Emnambithi/Ladysmith Local Municipality', 'Lepelle-Nkumpi Local Municipality', 'Umnambithi', 'George', 'Umtshezi Local Municipality', 'Nkomazi Municipality', 'Ba-Phalaborwa Local Municipality', 'Phokwane Local Municipality', 'Ratlou Local Municipality', 'Siyancuma Local Municipality', 'Kai\\xa0!Garib Local Municipality', 'king Sabata Dalindyebo', 'Mopani District Municipality', 'Johannesburg ', 'Mthonjaneni Local Municipality', 'Maphumolong Municipality', 'Ubuhlebezwe Municipality', 'Ulundi', 'Umvoti municipality', 'Emfuleni ', 'Mogale City Local Municipality', 'Msinga', 'Nyandeni', 'Mahikeng Local Municipality', 'Meyerton', 'Ga-Segonyana Local Municipality', 'Ubuhlebezwe', 'Elias Motsoaledi Local Municipality', 'Helderberg', 'Collins Chabane', 'Drakenstein Local Municipality', 'Makana', 'City of Johannesburg Metropolitan', 'Mangaung Metropolitan', 'Setsoto Local Municipality', 'Harry gwala district municiality', 'Lower Umfolozi', 'Tswelopele local municipality', 'Msukaligwa Local Municipality', 'City of Cape Town ', 'Umlalazi', 'Nquthu Local Municipality', 'Capricorn District Municipality', 'City of joburg ', 'Umsinga', 'Mogale City', 'Polokwane', 'Umzimkhulu Municipality', 'City of Tshwane', 'Knysna', 'Gqeberha', 'Ulundi Local Municipality', 'Gert Sibande District Municipality', 'Mthonjaneni', 'City of Joburg', 'Cederberg Local', 'Harry Gwala Municipality', 'Swartland Local Municipality', 'City of Tshwane ', 'Merafong City Local Municipality', 'Midvaal Local Municipality', 'Dr Nkosazane Dlamini Zuma', 'Rand west', 'Bergrivier Local Municipality', 'Mantsopa', 'Ekhuruleni', 'Makhuduthamaga Local Municipality', 'eThekwini municipality', 'Emthanjeni Local Municipality', 'City of Johannesburg ', 'Mandeni Local Municipality', 'Ekurhuleni ', 'Ratlou Local Municipality ', 'Tubatse']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:701: DtypeWarning: Columns (5,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
      "2023-08-02 21:15:51.703 INFO    visions.backends: Pandas backend loaded 1.4.4\n",
      "2023-08-02 21:15:51.729 INFO    visions.backends: Numpy backend loaded 1.21.5\n",
      "2023-08-02 21:15:51.799 INFO    visions.backends: Pyspark backend loaded 3.2.1\n",
      "2023-08-02 21:15:51.799 INFO    visions.backends: Python backend loaded\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:722: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Guid': '05dbba57-6046-ea11-833a-00155d326100', 'FullName': 'Zandile\\xa0Mgube ', 'IdNumber': '8712180875082', 'PersonalNumber': '0799734179', 'BirthDate': '1987-12-18', 'CountryOfCitizenship': 'South African', 'Age': '34', 'InactivityComments': None, 'MonthsSinceFranchisee': 42, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-02-02T22:00:00Z', 'Status': 'Active', 'Gender': 'Female', 'ProgrammeType': 'Full Week (Daymothers)', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': None, 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': '511c61ed-4fe6-ec11-8351-00155d326100'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'Guid': '46b0d437-5e46-ea11-833a-00155d326100', 'FullName': 'Nokwanda\\xa0Phungula ', 'IdNumber': '9110101294088', 'PersonalNumber': '0825839766', 'BirthDate': '1991-10-10', 'CountryOfCitizenship': 'South African', 'Age': '28', 'InactivityComments': 'Unable to reach', 'MonthsSinceFranchisee': 19, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-01-23T22:00:00Z', 'Status': 'Inactive', 'Gender': 'Female', 'ProgrammeType': 'Playgroup', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': 'Franchisee is uncontactable', 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': 'b4e5c772-26ef-e911-8326-0800274bb0e4'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'FullName': 'Lerato\\xa0Lehutso ', 'DateOfBirth': '1986-06-06', 'AreaOfOperation': 'Greater Taung', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6289ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Active', 'Guid': '31cc9b33-0a57-ea11-833a-00155d326100'}\n",
      "{'FullName': 'Funeka \\xa0Khoza ', 'DateOfBirth': None, 'AreaOfOperation': None, 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6a89ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Inactive', 'Guid': 'ae18e686-ac07-eb11-8343-00155d326100'}\n",
      "[{\"Name\":\"KZN Branch\",\"Guid\":\"5a6b40ec-1e4f-eb11-8345-00155d326100\"},{\"Name\":\"KET\",\"Guid\":\"3ed99252-d216-ec11-834c-00155d326100\"},{\"Name\":\"Lesedi Solar\",\"Guid\":\"e7b9933d-da35-ed11-8351-00155d326100\"},{\"Name\":\"Letsatsi Solar\",\"Guid\":\"821d436d-df4a-ed11-8355-00155d326100\"},{\"Name\":\"PPTrust\",\"Guid\":\"90b04590-6c96-ed11-8356-00155d326100\"},{\"Name\":\"NW Branch\",\"Guid\":\"6289ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"GP Branch\",\"Guid\":\"6a89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Khululeka\",\"Guid\":\"6c89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"LETCEE\",\"Guid\":\"6e89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"TREE\",\"Guid\":\"7089ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Siyakholwa\",\"Guid\":\"4b16de13-e386-e611-80ca-0050568109d5\"},{\"Name\":\"SAYM\",\"Guid\":\"474b1eed-e486-e611-80ca-0050568109d5\"},{\"Name\":\"Diaconia\",\"Guid\":\"cb6f6d1a-e686-e611-80ca-0050568109d5\"},{\"Name\":\"LIMA\",\"Guid\":\"f3e19b66-e786-e611-80ca-0050568109d5\"},{\"Name\":\"ELRU\",\"Guid\":\"20b4261f-e886-e611-80ca-0050568109d5\"},{\"Name\":\"Lesedi Educare Association\",\"Guid\":\"7911a744-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"3L Development\",\"Guid\":\"812aa76c-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Molteno\",\"Guid\":\"9093d385-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Penreach\",\"Guid\":\"0804f6ac-4584-e811-817d-0800274bb0e4\"}]\n",
      "(15174, 47)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b33d148621441c803ba1e5b59454ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\anaconda3.1\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n",
      "C:\\Users\\LazolaJavu\\anaconda3.1\\lib\\site-packages\\pandas_profiling\\model\\correlations.py:67: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/pandas-profiling/issues\n",
      "(include the error message: 'No data; `observed` has size 0.')\n",
      "  warnings.warn(\n",
      "2023-08-02 21:16:16.584 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:16.586 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.023 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.234 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.234 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.236 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.236 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.772 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.773 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.775 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:17.776 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:18.099 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:18.481 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:18.483 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.041 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.042 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.553 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.554 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.555 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.556 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.905 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:19.906 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.214 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.215 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.216 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.217 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.931 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.933 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.934 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:20.935 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.352 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 21:16:21.353 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.354 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.356 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.556 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.558 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.715 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.716 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.718 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:21.718 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:22.229 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:22.230 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:22.231 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2023-08-02 21:16:22.232 INFO    matplotlib.category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec81e08f7914172b224aa658a88cb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2777ec6123074b89bdf58169f8c48f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e1636d4ae14ea5b37c711480137ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71130bd5ae434db9bf2ff2eca47a1106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84304091787b466fa9bfbd5129fa3874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deac299e69644c991cf00c6db5129bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea7112c3c404087afd503b5aa0c5bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:1331: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_9904\\345988220.py:1418: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#################### ACTIVE CHILDREN #######################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "#################### INACTIVE CHILDREN############################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "# Save the data into a csv file.\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/children_data.csv\")\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "# from fbprophet import Prophet\n",
    "from functools import reduce\n",
    "# from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "start_date = datetime.date(2016, 1, 1)  # start date\n",
    "end_date = datetime.date(2031, 12, 19)  # end date\n",
    "num_days = (end_date - start_date).days + 1  # number of days between start and end dates\n",
    "\n",
    "# create array of values\n",
    "values = np.linspace(100, 100500, num_days)  # starting value of x and ending value of x_1\n",
    "# create array of values\n",
    "City_of_Johannesburg = np.linspace(332957, 332957, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Cape_Town = np.linspace(169677, 169677, num_days)  # starting value of x and ending value of x_1\n",
    "Polokwane = np.linspace(72052, 72052, num_days)  # starting value of x and ending value of x_1\n",
    "Ekurhuleni = np.linspace(209777, 209777, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Tshwane = np.linspace(158393, 158393, num_days)  # starting value of x and ending value of x_1\n",
    "Bushbuckridge = np.linspace(40688, 40688, num_days)  # starting value of x and ending value of x_1\n",
    "Ehlanzeni = np.linspace(116582, 116582, num_days)  # starting value of x and ending value of x_1\n",
    "Bojanala = np.linspace(151168, 151168, num_days)  # starting value of x and ending value of x_1\n",
    "Sedibeng = np.linspace(79595, 79595, num_days)  # starting value of x and ending value of x_1\n",
    "Madibeng = np.linspace(54912, 54912, num_days)  # starting value of x and ending value of x_1\n",
    "Collins_Chabane = np.linspace(50667, 50667, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Mbombela = np.linspace(79542, 79542, num_days)  # starting value of x and ending value of x_1\n",
    "Nyandeni = np.linspace(47504, 47504, num_days)  # starting value of x and ending value of x_1\n",
    "Rustenburg = np.linspace(68991, 68991, num_days)  # starting value of x and ending value of x_1\n",
    "Buffalo_City = np.linspace(72685, 72685, num_days)  # starting value of x and ending value of x_1\n",
    "King_Sabata_Dalindyebo = np.linspace(59373, 59373, num_days)  # starting value of x and ending value of x_1\n",
    "Nkomazi = np.linspace(53994, 53994, num_days)  # starting value of x and ending value of x_1\n",
    "Emfuleni = np.linspace(62590, 62590, num_days)  # starting value of x and ending value of x_1\n",
    "Nelson_Mandela_Bay = np.linspace(65905, 65905, num_days)  # starting value of x and ending value of x_1\n",
    "Thembisile = np.linspace(50006, 50006, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Matlosana = np.linspace(45887, 45887, num_days)  # starting value of x and ending value of x_1\n",
    "\n",
    "\n",
    "# create array of dates\n",
    "dates = [start_date + datetime.timedelta(days=i) for i in range(num_days)]\n",
    "\n",
    "# create dataframe\n",
    "data_muni_gap = pd.DataFrame({'date': dates, 'value': values, 'City of Johannesburg':City_of_Johannesburg,\n",
    "                   'City of Cape Town':City_of_Cape_Town, 'Polokwane':Polokwane, 'Ekurhuleni':Ekurhuleni,\n",
    "                   'City of Tshwane':City_of_Tshwane,'Bushbuckridge':Bushbuckridge,'Ehlanzeni':Ehlanzeni,'Bojanala':Bojanala,\n",
    "                   'Sedibeng':Sedibeng,'Local Municipality of Madibeng':Madibeng,'Collins Chabane':Collins_Chabane,'City of Mbombela':City_of_Mbombela,\n",
    "                   'Nyandeni':Nyandeni,'Rustenburg':Rustenburg,'Buffalo City':Buffalo_City,'King Sabata Dalindyebo':King_Sabata_Dalindyebo,\n",
    "                   'Nkomazi':Nkomazi,'Emfuleni':Emfuleni,'Nelson Mandela Bay':Nelson_Mandela_Bay,'Thembisile':Thembisile, 'City of Matlosana':City_of_Matlosana})\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "data = data[['Guid', 'FullName', 'StartDate', 'InactiveDate', 'Status',\n",
    "       'Franchisee.FullName', 'Franchisee.IdNumber', 'Franchisee.Province',\n",
    "       'Franchisee.ProgrammeType', 'Franchisee.Guid']]\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "data_children = data.copy()\n",
    "\n",
    "# read_csv file:\n",
    "site_municipalities = pd.read_excel(r\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/Site Municipality.xlsx\")\n",
    "\n",
    "# read_csv file:\n",
    "data_franchisee = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "data_franchisee =data_franchisee[['Guid', 'FullName', 'IdNumber',\n",
    "       'Gender', 'ProgrammeType', 'EthnicGroup', 'Province',\n",
    "       'ReasonForLeaving', 'Principal', 'SiteAddress', 'Franchisor.Guid',\n",
    "       'Coach.Guid', 'SiteAddress.Municipality',\n",
    "       'Franchisor']]\n",
    "# # Merge the excel file with the data we pulled from\n",
    "merge_df = pd.merge(data_children, data_franchisee, left_on = 'Franchisee.IdNumber', right_on = 'IdNumber')\n",
    "merge_df = pd.merge(merge_df, site_municipalities, left_on = 'Franchisee.IdNumber', right_on = 'ID Number')\n",
    "# Data copy\n",
    "data = merge_df.copy()\n",
    "# select the franchisees that are active.\n",
    "data_cpt_active = data[data['Status'] == 'Active']\n",
    "# Use the province column to check the model performance by Province\n",
    "data_cpt_muni = data_cpt_active[['StartDate','Site Municipality']]\n",
    "# Now let us count the data of the site municipality\n",
    "data_cpt_muni['Site Municipality'].value_counts().to_frame()\n",
    "\n",
    "# Drop NaN values from the 'Site Municipality' column\n",
    "data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n",
    "\n",
    "# Get unique values from the 'Site Municipality' column\n",
    "unique_values = list(set(data_cpt_active['Site Municipality']))\n",
    "\n",
    "print(unique_values)\n",
    "\n",
    "municipality_list = unique_values\n",
    "\n",
    "data_cpt_muni = data_cpt_muni.loc[data['Site Municipality'].isin(municipality_list)]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_cpt_muni['Site Municipality'])\n",
    "# merge the datasets\n",
    "data_cpt_muni = data_cpt_muni.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_cpt_muni['id'] = data_cpt_muni['StartDate']\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['StartDate']\n",
    "# Reset the index\n",
    "data_cpt_muni.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['Site Municipality']\n",
    "# View the columns\n",
    "municipality_list_2 = list(data_cpt_muni.columns)\n",
    "# Create a copy of the data\n",
    "df_grouped = data_cpt_muni.copy()\n",
    "# Rearrange the columns from the dataframe\n",
    "df_grouped = df_grouped[municipality_list_2]\n",
    "df_grouped.reset_index()\n",
    "# convert 'date' column to date format\n",
    "df_grouped['id'] = pd.to_datetime(df_grouped['id'], errors='coerce')\n",
    "\n",
    "# Convert start_date to datetime object\n",
    "start_date = datetime.datetime.combine(datetime.date(2016, 1, 1), datetime.datetime.min.time())\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "\n",
    "# Filter the DataFrame based on the date range\n",
    "df_grouped = df_grouped[(df_grouped['id'] >= start_date) & (df_grouped['id'] <= end_date)]\n",
    "# Define function to create time series\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')[['id', column_name]]\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    data = data.rename(columns={'id': 'ds', column_name: 'y'})\n",
    "    data = data.dropna().drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "# Apply function to create time series for selected cities\n",
    "cities = municipality_list\n",
    "df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
    "\n",
    "# Rename the columns\n",
    "df_grouped_basic.columns = ['ds'] + municipality_list\n",
    "\n",
    "\n",
    "df_grouped_basic.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/df_grouped_basic.csv\")\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "# import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from streamlit import components\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import savgol_filter\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import folium\n",
    "from streamlit_folium import folium_static\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Libraries needed for the tutorial\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Username of your GitHub account\n",
    "\n",
    "username = 'lazola.javu@gmail.com'\n",
    "\n",
    "# Personal Access Token (PAO) from your GitHub account\n",
    "\n",
    "# token = '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'\n",
    "\n",
    "token = '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'\n",
    "\n",
    "########### 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 #################### Token doesn't expire\n",
    "\n",
    "# Creates a re-usable session object with your creds in-built\n",
    "github_session = requests.Session()\n",
    "github_session.auth = (username, token)\n",
    "\n",
    "url_4 = \"https://raw.githubusercontent.com/LazolaJavu/Time-Series-Forecasting/main/Data/external/ECD_Data.csv?token=GHSAT0AAAAAACBIG46DYRXUHQIDWHOEL6B6ZEQEMQA\"\n",
    "ecd_census_data = github_session.get(url_4).content\n",
    "\n",
    "# Reading the downloaded content and making it a pandas dataframe\n",
    "ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
    "# Read the shape file\n",
    "shape_file = gpd.read_file(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\")\n",
    "# Read the shape file\n",
    "gdf = shape_file[shape_file['geonunit'] == 'South Africa']\n",
    "# Read the children data\n",
    "df = pd.read_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv')\n",
    "# Rename the column table\n",
    "df['Province'] = df['Franchisee.Province']\n",
    "# Group the data by province and count the values\n",
    "province_counts = df['Province'].value_counts().reset_index()\n",
    "province_counts.columns = ['Province', 'Counts']\n",
    "merged_data = gdf.merge(province_counts, left_on='woe_name', right_on='Province', how='left')\n",
    "# Copy the merged data\n",
    "gdf = merged_data.copy()\n",
    "# Save a csv file:\n",
    "gdf.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/gdf.csv\")\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "print(data.shape)\n",
    "# Create the Profile Report\n",
    "profile = ProfileReport(data, title = \"SmartStart Franchisees EDA\")\n",
    "# Save as an html file\n",
    "profile.to_file(\"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/franchisee_report.html\")\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "################################### GET CHILDREN DATA #######################################\n",
    "def get_franchisee_data(franchisor_value):\n",
    "\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\"Guid\",\n",
    "    \"FullName\",\n",
    "    \"FirstName\",\n",
    "    \"Surname\",\n",
    "    \"IdNumber\",\n",
    "    \"AllergyType\",\n",
    "    \"DisabilityType\",\n",
    "    \"HealthConditions\",\n",
    "    \"EmergencyContactNumber\",\n",
    "    \"EmergencyContactFullName\",\n",
    "    \"EmergencyContactFirstName\",\n",
    "    \"EmergencyContactSurname\",\n",
    "    \"AlternativePickupFirstName\",\n",
    "    \"AlternativePickupSurname\",\n",
    "    \"AlternativePickupContactNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"StartDate\",\n",
    "    \"HasAllergy\",\n",
    "    \"HasDisability\",\n",
    "    \"CaregiverPopiaConsent\",\n",
    "    \"CaregiverPhotographyAndFilmingConsent\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"HasIdNumber\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"HomeLanguage\",\n",
    "    \"GrantType\",\n",
    "    \"PlaygroupGroup\",\n",
    "    \"InactiveReason\",\n",
    "    \"Franchisee\",\n",
    "    \"Caregiver\",\n",
    "    \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\"Guid\",\n",
    "    \"FullName\",\n",
    "    \"FirstName\",\n",
    "    \"Surname\",\n",
    "    \"IdNumber\",\n",
    "    \"AllergyType\",\n",
    "    \"DisabilityType\",\n",
    "    \"HealthConditions\",\n",
    "    \"EmergencyContactNumber\",\n",
    "    \"EmergencyContactFullName\",\n",
    "    \"EmergencyContactFirstName\",\n",
    "    \"EmergencyContactSurname\",\n",
    "    \"AlternativePickupFirstName\",\n",
    "    \"AlternativePickupSurname\",\n",
    "    \"AlternativePickupContactNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"StartDate\",\n",
    "    \"HasAllergy\",\n",
    "    \"HasDisability\",\n",
    "    \"CaregiverPopiaConsent\",\n",
    "    \"CaregiverPhotographyAndFilmingConsent\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"HasIdNumber\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"HomeLanguage\",\n",
    "    \"GrantType\",\n",
    "    \"PlaygroupGroup\",\n",
    "    \"InactiveReason\",\n",
    "    \"Franchisee\",\n",
    "    \"Caregiver\",\n",
    "    \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "\n",
    "profile = ProfileReport(data, minimal=True)\n",
    "profile.to_file(output_file=\"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/Children_report_report.html\")\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "data_mod = data_mod[data_mod['id'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroup.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstart.csv')\n",
    "\n",
    "##################################################################### Franchisee #############################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data_mod = data_mod[data_mod['id'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data_mod['id'] = data_mod['id'].dt.strftime('%Y-%m-%d')\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd_franchisees.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother_franchisees.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroupdata_franchisees.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstartdata_franchisees.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Franchisee.Province' ]]\n",
    "df_sd = data[['StartDate', 'Franchisee.Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Franchisee.Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Franchisee.Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Franchisee.Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Franchisee.Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS.csv')\n",
    "\n",
    "############################################################################## FRANCHISEES #################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data = data[data['StartDate'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data['StartDate'] = data['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Province' ]]\n",
    "df_sd = data[['StartDate', 'Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS_franchisees.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "########################################################################### CHILDREN DATA ##########################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_children_data.csv')\n",
    "\n",
    "################################################################# franchisees Data ##########################################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Convert the 'StartDate' column to datetime format\n",
    "df_dates['StartDate'] = pd.to_datetime(df_dates['StartDate'], errors='coerce')\n",
    "\n",
    "# Extract only the date portion from the 'StartDate' column\n",
    "df_dates['StartDate'] = df_dates['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_Franchisee_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9206fe53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T19:37:29.108733Z",
     "start_time": "2023-08-02T19:34:52.022849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date         Value\n",
      "0    2021-07-01   1033.000000\n",
      "1    2021-07-02   1032.260989\n",
      "2    2021-07-03   1031.521978\n",
      "3    2021-07-04   1030.782967\n",
      "4    2021-07-05   1030.043956\n",
      "...         ...           ...\n",
      "3647 2031-06-26  42936.802198\n",
      "3648 2031-06-27  42941.351648\n",
      "3649 2031-06-28  42945.901099\n",
      "3650 2031-06-29  42950.450549\n",
      "3651 2031-06-30  42955.000000\n",
      "\n",
      "[3652 rows x 2 columns]\n",
      "           Date         Value\n",
      "0    2021-07-01   3500.000000\n",
      "1    2021-07-02   3503.357143\n",
      "2    2021-07-03   3506.714286\n",
      "3    2021-07-04   3510.071429\n",
      "4    2021-07-05   3513.428571\n",
      "...         ...           ...\n",
      "3647 2031-06-26  42936.802198\n",
      "3648 2031-06-27  42941.351648\n",
      "3649 2031-06-28  42945.901099\n",
      "3650 2031-06-29  42950.450549\n",
      "3651 2031-06-30  42955.000000\n",
      "\n",
      "[3652 rows x 2 columns]\n",
      "           Date          Value\n",
      "0    2021-07-01   45000.000000\n",
      "1    2021-07-02   45018.747253\n",
      "2    2021-07-03   45037.494505\n",
      "3    2021-07-04   45056.241758\n",
      "4    2021-07-05   45074.989011\n",
      "...         ...            ...\n",
      "3647 2031-06-26  600000.000000\n",
      "3648 2031-06-27  600000.000000\n",
      "3649 2031-06-28  600000.000000\n",
      "3650 2031-06-29  600000.000000\n",
      "3651 2031-06-30  600000.000000\n",
      "\n",
      "[3652 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Province'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Province'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9904\\1526792858.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Status'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Active'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m \u001b[0mvalue_counts_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Province'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_counts_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_counts_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Province'"
     ]
    }
   ],
   "source": [
    "################################################ SMARTSTART GHS DATA ################################################\n",
    "############################################### FRANCHISEE GHS DATA #################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_date(begin_date, finish_date, sv, ev):\n",
    "    # Define the start and end dates\n",
    "    start_date = pd.to_datetime(begin_date, format=\"%d-%m-%Y\")\n",
    "    end_date = pd.to_datetime(finish_date, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Define the start and end values\n",
    "    start_value = sv\n",
    "    end_value = ev\n",
    "\n",
    "    # Create a range of dates\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Generate random increasing values\n",
    "    num_values = len(dates)\n",
    "    random_values = np.linspace(start_value, end_value, num_values)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Date': dates, 'Value': random_values})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the individual dataframes\n",
    "df1 = create_date('01-07-2021', '30-06-2022', 1033, 764)\n",
    "df2 = create_date('01-07-2022', '30-06-2023', 764, 1440)\n",
    "df3 = create_date('01-07-2023', '30-06-2024', 1440, 1842)\n",
    "df4 = create_date('01-07-2024', '30-06-2025', 1842, 17979)\n",
    "df5 = create_date('01-07-2025', '30-06-2026', 17979, 24290)\n",
    "df6 = create_date('01-07-2026', '30-06-2027', 24290, 31476)\n",
    "df7 = create_date('01-07-2027', '30-06-2028', 31476, 38851)\n",
    "df8 = create_date('01-07-2028', '30-06-2029', 38851, 42326)\n",
    "df9 = create_date('01-07-2029', '30-06-2030', 42326, 41299)\n",
    "df10 = create_date('01-07-2030', '30-06-2031', 41299, 42955)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_franchisees = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_franchisees.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_franchisees)\n",
    "\n",
    "df_GHS_franchisees.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_franchisees_Modality.csv\")\n",
    "########################################## CHILDREN GHS DATA #####################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_date(begin_date, finish_date, sv, ev):\n",
    "    # Define the start and end dates\n",
    "    start_date = pd.to_datetime(begin_date, format=\"%d-%m-%Y\")\n",
    "    end_date = pd.to_datetime(finish_date, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Define the start and end values\n",
    "    start_value = sv\n",
    "    end_value = ev\n",
    "\n",
    "    # Create a range of dates\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Generate random increasing values\n",
    "    num_values = len(dates)\n",
    "    random_values = np.linspace(start_value, end_value, num_values)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Date': dates, 'Value': random_values})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the individual dataframes\n",
    "df1 = create_date('01-07-2021', '30-06-2022', 3500, 4722)\n",
    "df2 = create_date('01-07-2022', '30-06-2023', 4722, 8219)\n",
    "df3 = create_date('01-07-2023', '30-06-2024', 8219, 11149)\n",
    "df4 = create_date('01-07-2024', '30-06-2025', 11149, 17979)\n",
    "df5 = create_date('01-07-2025', '30-06-2026', 17979, 24290)\n",
    "df6 = create_date('01-07-2026', '30-06-2027', 24290, 31476)\n",
    "df7 = create_date('01-07-2027', '30-06-2028', 31476, 38851)\n",
    "df8 = create_date('01-07-2028', '30-06-2029', 38851, 42326)\n",
    "df9 = create_date('01-07-2029', '30-06-2030', 42326, 41299)\n",
    "df10 = create_date('01-07-2030', '30-06-2031', 41299, 42955)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_franchisees = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_franchisees.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_franchisees)\n",
    "\n",
    "df_GHS_franchisees.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_franchisees.csv\")\n",
    "\n",
    "df1 = create_date('01-07-2021','30-06-2022',45000,51824)\n",
    "df2 = create_date('01-07-2022','30-06-2023',51824,76800)\n",
    "df3 = create_date('01-07-2023','30-06-2024',76800,130000)\n",
    "df4 = create_date('01-07-2024','30-06-2025',130000,170000)\n",
    "df5 = create_date('01-07-2025','30-06-2026',170000,294000)\n",
    "df6 = create_date('01-07-2026','30-06-2027',294000,392000)\n",
    "df7 = create_date('01-07-2027','30-06-2028',392000,561000)\n",
    "df8 = create_date('01-07-2028','30-06-2029',561000,612000)\n",
    "df9 = create_date('01-07-2029','30-06-2030',612000,600000)\n",
    "df10 = create_date('01-07-2030','30-06-2031',600000,600000)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_children = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_children.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_children)\n",
    "\n",
    "df_GHS_children.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_children.csv\")\n",
    "################################ Download Children DATA ########################################\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#################### ACTIVE CHILDREN #######################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "#################### INACTIVE CHILDREN############################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "# Save the data into a csv file.\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/children_data.csv\")\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "######################################################################## Read shapefile################################\n",
    "#set up the file path and read the shapefile data\n",
    "fp = \"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Live Data Connection/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shx\"\n",
    "map_df = gpd.read_file(fp)\n",
    "# map_df.to_crs(pyproj.CRS.from_epsg(4326), inplace=True)\n",
    "map_df = map_df[map_df['admin']=='South Africa']\n",
    "map_df = map_df[['name','adm1_code', 'diss_me','geometry']]\n",
    "\n",
    "merged = map_df.copy()\n",
    "merged\n",
    "data = data[data['Status'] == 'Active']\n",
    "value_counts_df = data['Province'].value_counts().reset_index()\n",
    "print(value_counts_df)\n",
    "merged = pd.merge(merged, value_counts_df, how = 'left',left_on = 'name', right_on='index')\n",
    "# Assuming you have a GeoDataFrame named 'df' that you want to save as GeoJSON\n",
    "output_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map_children.geojson\"\n",
    "# Save the GeoDataFrame as GeoJSON\n",
    "merged.to_file(output_fp, driver='GeoJSON')\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "input_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map_children.geojson\"\n",
    "gdf = gpd.read_file(input_fp)\n",
    "\n",
    "#################################################### DOWNLOAD FRANCHISEE DATA #########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt #if using matplotlib\n",
    "import plotly.express as px #if using plotly\n",
    "import geopandas as gpd\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv')\n",
    "\n",
    "#set up the file path and read the shapefile data\n",
    "fp = \"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Live Data Connection/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shx\"\n",
    "map_df = gpd.read_file(fp)\n",
    "# map_df.to_crs(pyproj.CRS.from_epsg(4326), inplace=True)\n",
    "map_df = map_df[map_df['admin']=='South Africa']\n",
    "map_df = map_df[['name','adm1_code', 'diss_me','geometry']]\n",
    "\n",
    "merged = map_df.copy()\n",
    "merged\n",
    "data = data[data['Status'] == 'Active']\n",
    "value_counts_df = data['Province'].value_counts().reset_index()\n",
    "value_counts_df\n",
    "merged = pd.merge(merged, value_counts_df, how = 'left',left_on = 'name', right_on='index')\n",
    "# Assuming you have a GeoDataFrame named 'df' that you want to save as GeoJSON\n",
    "output_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map.geojson\"\n",
    "# Save the GeoDataFrame as GeoJSON\n",
    "merged.to_file(output_fp, driver='GeoJSON')\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "input_fp =  \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map.geojson\"\n",
    "gdf = gpd.read_file(input_fp)\n",
    "\n",
    "############################################### CHILDREN BY MUNICIPALITY ###################################\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "# from fbprophet import Prophet\n",
    "from functools import reduce\n",
    "# from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "start_date = datetime.date(2016, 1, 1)  # start date\n",
    "end_date = datetime.date(2031, 12, 19)  # end date\n",
    "num_days = (end_date - start_date).days + 1  # number of days between start and end dates\n",
    "\n",
    "# create array of values\n",
    "values = np.linspace(100, 100500, num_days)  # starting value of x and ending value of x_1\n",
    "# create array of values\n",
    "City_of_Johannesburg = np.linspace(332957, 332957, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Cape_Town = np.linspace(169677, 169677, num_days)  # starting value of x and ending value of x_1\n",
    "Polokwane = np.linspace(72052, 72052, num_days)  # starting value of x and ending value of x_1\n",
    "Ekurhuleni = np.linspace(209777, 209777, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Tshwane = np.linspace(158393, 158393, num_days)  # starting value of x and ending value of x_1\n",
    "Bushbuckridge = np.linspace(40688, 40688, num_days)  # starting value of x and ending value of x_1\n",
    "Ehlanzeni = np.linspace(116582, 116582, num_days)  # starting value of x and ending value of x_1\n",
    "Bojanala = np.linspace(151168, 151168, num_days)  # starting value of x and ending value of x_1\n",
    "Sedibeng = np.linspace(79595, 79595, num_days)  # starting value of x and ending value of x_1\n",
    "Madibeng = np.linspace(54912, 54912, num_days)  # starting value of x and ending value of x_1\n",
    "Collins_Chabane = np.linspace(50667, 50667, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Mbombela = np.linspace(79542, 79542, num_days)  # starting value of x and ending value of x_1\n",
    "Nyandeni = np.linspace(47504, 47504, num_days)  # starting value of x and ending value of x_1\n",
    "Rustenburg = np.linspace(68991, 68991, num_days)  # starting value of x and ending value of x_1\n",
    "Buffalo_City = np.linspace(72685, 72685, num_days)  # starting value of x and ending value of x_1\n",
    "King_Sabata_Dalindyebo = np.linspace(59373, 59373, num_days)  # starting value of x and ending value of x_1\n",
    "Nkomazi = np.linspace(53994, 53994, num_days)  # starting value of x and ending value of x_1\n",
    "Emfuleni = np.linspace(62590, 62590, num_days)  # starting value of x and ending value of x_1\n",
    "Nelson_Mandela_Bay = np.linspace(65905, 65905, num_days)  # starting value of x and ending value of x_1\n",
    "Thembisile = np.linspace(50006, 50006, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Matlosana = np.linspace(45887, 45887, num_days)  # starting value of x and ending value of x_1\n",
    "\n",
    "\n",
    "# create array of dates\n",
    "dates = [start_date + datetime.timedelta(days=i) for i in range(num_days)]\n",
    "\n",
    "# create dataframe\n",
    "data_muni_gap = pd.DataFrame({'date': dates, 'value': values, 'City of Johannesburg':City_of_Johannesburg,\n",
    "                   'City of Cape Town':City_of_Cape_Town, 'Polokwane':Polokwane, 'Ekurhuleni':Ekurhuleni,\n",
    "                   'City of Tshwane':City_of_Tshwane,'Bushbuckridge':Bushbuckridge,'Ehlanzeni':Ehlanzeni,'Bojanala':Bojanala,\n",
    "                   'Sedibeng':Sedibeng,'Local Municipality of Madibeng':Madibeng,'Collins Chabane':Collins_Chabane,'City of Mbombela':City_of_Mbombela,\n",
    "                   'Nyandeni':Nyandeni,'Rustenburg':Rustenburg,'Buffalo City':Buffalo_City,'King Sabata Dalindyebo':King_Sabata_Dalindyebo,\n",
    "                   'Nkomazi':Nkomazi,'Emfuleni':Emfuleni,'Nelson Mandela Bay':Nelson_Mandela_Bay,'Thembisile':Thembisile, 'City of Matlosana':City_of_Matlosana})\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "data = data[['Guid', 'FullName', 'StartDate', 'InactiveDate', 'Status',\n",
    "       'Franchisee.FullName', 'Franchisee.IdNumber', 'Franchisee.Province',\n",
    "       'Franchisee.ProgrammeType', 'Franchisee.Guid']]\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "data_children = data.copy()\n",
    "\n",
    "# read_csv file:\n",
    "site_municipalities = pd.read_excel(r\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/Site Municipality.xlsx\")\n",
    "\n",
    "# read_csv file:\n",
    "data_franchisee = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "data_franchisee =data_franchisee[['Guid', 'FullName', 'IdNumber',\n",
    "       'Gender', 'ProgrammeType', 'EthnicGroup', 'Province',\n",
    "       'ReasonForLeaving', 'Principal', 'SiteAddress', 'Franchisor.Guid',\n",
    "       'Coach.Guid', 'SiteAddress.Municipality',\n",
    "       'Franchisor']]\n",
    "# # Merge the excel file with the data we pulled from\n",
    "merge_df = pd.merge(data_children, data_franchisee, left_on = 'Franchisee.IdNumber', right_on = 'IdNumber')\n",
    "merge_df = pd.merge(merge_df, site_municipalities, left_on = 'Franchisee.IdNumber', right_on = 'ID Number')\n",
    "# Data copy\n",
    "data = merge_df.copy()\n",
    "# select the franchisees that are active.\n",
    "data_cpt_active = data[data['Status'] == 'Active']\n",
    "# Use the province column to check the model performance by Province\n",
    "data_cpt_muni = data_cpt_active[['StartDate','Site Municipality']]\n",
    "# Now let us count the data of the site municipality\n",
    "data_cpt_muni['Site Municipality'].value_counts().to_frame()\n",
    "\n",
    "# Drop NaN values from the 'Site Municipality' column\n",
    "data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n",
    "\n",
    "# Get unique values from the 'Site Municipality' column\n",
    "unique_values = list(set(data_cpt_active['Site Municipality']))\n",
    "\n",
    "print(unique_values)\n",
    "\n",
    "municipality_list = unique_values\n",
    "\n",
    "data_cpt_muni = data_cpt_muni.loc[data['Site Municipality'].isin(municipality_list)]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_cpt_muni['Site Municipality'])\n",
    "# merge the datasets\n",
    "data_cpt_muni = data_cpt_muni.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_cpt_muni['id'] = data_cpt_muni['StartDate']\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['StartDate']\n",
    "# Reset the index\n",
    "data_cpt_muni.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['Site Municipality']\n",
    "# View the columns\n",
    "municipality_list_2 = list(data_cpt_muni.columns)\n",
    "# Create a copy of the data\n",
    "df_grouped = data_cpt_muni.copy()\n",
    "# Rearrange the columns from the dataframe\n",
    "df_grouped = df_grouped[municipality_list_2]\n",
    "df_grouped.reset_index()\n",
    "# convert 'date' column to date format\n",
    "df_grouped['id'] = pd.to_datetime(df_grouped['id'], errors='coerce')\n",
    "\n",
    "# Convert start_date to datetime object\n",
    "start_date = datetime.datetime.combine(datetime.date(2016, 1, 1), datetime.datetime.min.time())\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "\n",
    "# Filter the DataFrame based on the date range\n",
    "df_grouped = df_grouped[(df_grouped['id'] >= start_date) & (df_grouped['id'] <= end_date)]\n",
    "# Define function to create time series\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')[['id', column_name]]\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    data = data.rename(columns={'id': 'ds', column_name: 'y'})\n",
    "    data = data.dropna().drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "# Apply function to create time series for selected cities\n",
    "cities = municipality_list\n",
    "df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
    "\n",
    "# Rename the columns\n",
    "df_grouped_basic.columns = ['ds'] + municipality_list\n",
    "\n",
    "\n",
    "df_grouped_basic.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/df_grouped_basic.csv\")\n",
    "\n",
    "######################################### ECD CENSUS DATA CONVERSION ###################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ECD_Data.xlsx')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ECD_Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ FEATURE ENGINEERING - OVERALL ##############################\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "########################################################################### CHILDREN DATA ##########################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_children_data.csv')\n",
    "\n",
    "################################################################# franchisees Data ##########################################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Convert the 'StartDate' column to datetime format\n",
    "df_dates['StartDate'] = pd.to_datetime(df_dates['StartDate'], errors='coerce')\n",
    "\n",
    "# Extract only the date portion from the 'StartDate' column\n",
    "df_dates['StartDate'] = df_dates['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_Franchisee_data.csv')\n",
    "\n",
    "########################################### F.E. PROGRAMME TYPE ####################################################\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "data_mod = data_mod[data_mod['id'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroup.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstart.csv')\n",
    "\n",
    "##################################################################### Franchisee #############################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data_mod = data_mod[data_mod['id'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data_mod['id'] = data_mod['id'].dt.strftime('%Y-%m-%d')\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd_franchisees.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother_franchisees.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroupdata_franchisees.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstartdata_franchisees.csv')\n",
    "\n",
    "############################################# F.E. PROVINCES ###########################################\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Franchisee.Province' ]]\n",
    "df_sd = data[['StartDate', 'Franchisee.Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Franchisee.Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Franchisee.Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Franchisee.Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Franchisee.Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS.csv')\n",
    "\n",
    "############################################################################## FRANCHISEES #################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data = data[data['StartDate'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data['StartDate'] = data['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Province' ]]\n",
    "df_sd = data[['StartDate', 'Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS_franchisees.csv')\n",
    "\n",
    "######################################################### DATA VISUALISATION #################################\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "# import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from streamlit import components\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import savgol_filter\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import folium\n",
    "from streamlit_folium import folium_static\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Libraries needed for the tutorial\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Username of your GitHub account\n",
    "\n",
    "username = 'lazola.javu@gmail.com'\n",
    "\n",
    "# Personal Access Token (PAO) from your GitHub account\n",
    "\n",
    "# token = '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'\n",
    "\n",
    "token = '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'\n",
    "\n",
    "########### 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 #################### Token doesn't expire\n",
    "\n",
    "# Creates a re-usable session object with your creds in-built\n",
    "github_session = requests.Session()\n",
    "github_session.auth = (username, token)\n",
    "\n",
    "url_4 = \"https://raw.githubusercontent.com/LazolaJavu/Time-Series-Forecasting/main/Data/external/ECD_Data.csv?token=GHSAT0AAAAAACBIG46DYRXUHQIDWHOEL6B6ZEQEMQA\"\n",
    "ecd_census_data = github_session.get(url_4).content\n",
    "\n",
    "# Reading the downloaded content and making it a pandas dataframe\n",
    "ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
    "# Read the shape file\n",
    "shape_file = gpd.read_file(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\")\n",
    "# Read the shape file\n",
    "gdf = shape_file[shape_file['geonunit'] == 'South Africa']\n",
    "# Read the children data\n",
    "df = pd.read_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv')\n",
    "# Rename the column table\n",
    "df['Province'] = df['Franchisee.Province']\n",
    "# Group the data by province and count the values\n",
    "province_counts = df['Province'].value_counts().reset_index()\n",
    "province_counts.columns = ['Province', 'Counts']\n",
    "merged_data = gdf.merge(province_counts, left_on='woe_name', right_on='Province', how='left')\n",
    "# Copy the merged data\n",
    "gdf = merged_data.copy()\n",
    "# Save a csv file:\n",
    "gdf.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/gdf.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
