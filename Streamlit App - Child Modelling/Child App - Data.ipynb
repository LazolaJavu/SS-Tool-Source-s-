{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454ce906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T16:41:18.928755Z",
     "start_time": "2023-08-21T16:31:19.028874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 500\n",
      "{'Guid': '46b0d437-5e46-ea11-833a-00155d326100', 'FullName': 'Nokwanda\\xa0Phungula ', 'IdNumber': '9110101294088', 'PersonalNumber': '0825839766', 'BirthDate': '1991-10-10', 'CountryOfCitizenship': 'South African', 'Age': '28', 'InactivityComments': 'Unable to reach', 'MonthsSinceFranchisee': 19, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-01-23T22:00:00Z', 'InactiveDate': '2021-08-01T22:00:00Z', 'Status': 'Inactive', 'Gender': 'Female', 'ProgrammeType': 'Playgroup', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': 'Franchisee is uncontactable', 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': 'b4e5c772-26ef-e911-8326-0800274bb0e4'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'FullName': 'Lerato\\xa0Lehutso ', 'DateOfBirth': '1986-06-06', 'AreaOfOperation': 'Greater Taung', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6289ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Active', 'Guid': '31cc9b33-0a57-ea11-833a-00155d326100'}\n",
      "{'FullName': 'Amanda Emily\\xa0Mapolisa ', 'DateOfBirth': None, 'AreaOfOperation': 'Orange farm', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6a89ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Inactive', 'Guid': 'b0ebc959-a507-eb11-8343-00155d326100'}\n",
      "[{\"Name\":\"KZN Branch\",\"Guid\":\"5a6b40ec-1e4f-eb11-8345-00155d326100\"},{\"Name\":\"KET\",\"Guid\":\"3ed99252-d216-ec11-834c-00155d326100\"},{\"Name\":\"Lesedi Solar\",\"Guid\":\"e7b9933d-da35-ed11-8351-00155d326100\"},{\"Name\":\"Letsatsi Solar\",\"Guid\":\"821d436d-df4a-ed11-8355-00155d326100\"},{\"Name\":\"PPTrust\",\"Guid\":\"90b04590-6c96-ed11-8356-00155d326100\"},{\"Name\":\"NW Branch\",\"Guid\":\"6289ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"GP Branch\",\"Guid\":\"6a89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Khululeka\",\"Guid\":\"6c89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"LETCEE\",\"Guid\":\"6e89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"TREE\",\"Guid\":\"7089ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Siyakholwa\",\"Guid\":\"4b16de13-e386-e611-80ca-0050568109d5\"},{\"Name\":\"SAYM\",\"Guid\":\"474b1eed-e486-e611-80ca-0050568109d5\"},{\"Name\":\"Diaconia\",\"Guid\":\"cb6f6d1a-e686-e611-80ca-0050568109d5\"},{\"Name\":\"LIMA\",\"Guid\":\"f3e19b66-e786-e611-80ca-0050568109d5\"},{\"Name\":\"ELRU\",\"Guid\":\"20b4261f-e886-e611-80ca-0050568109d5\"},{\"Name\":\"Lesedi Educare Association\",\"Guid\":\"7911a744-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"3L Development\",\"Guid\":\"812aa76c-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Molteno\",\"Guid\":\"9093d385-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Penreach\",\"Guid\":\"0804f6ac-4584-e811-817d-0800274bb0e4\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nelson Mandela Bay Metropolitan Municipality', 'Ventersdorp Local Municipality', 'Mbombela Local Municipality', 'Cederberg Local Municipality', 'George Local Municipality', 'City of Matlosana Local Municipality', 'Kgatelopele Local Municipality', 'Raymond Mhlaba', 'Msinga Local Municipality', 'Sol Plaatje Local Municipality', 'City of Cape Town', 'Tshwane municipality', 'City of Johannesburg', 'Saldanha Bay Local Municipality', 'Emthanjeni Local Municipality', 'Enoch Mgijima', 'Tshwane', 'COE', 'Emfuleni Local Municipality', 'Johannesburg', 'Bitou Local Municipality', 'Ditsobotla Local Municipality', 'Joe Slovo', 'uMhlathuze Local Municipality', 'Midvaal', 'tshwane', 'Stellenbosch Local Municipality', 'Magareng Local Municipality', 'Greater Giyani Local Municipality', 'Ekurhuleni Metropolitan Municipality', 'Nkomazi Local Municipality', 'City of Tshwane Metropolitan Municipality', 'King Sabata Dalindyebo Local Municipality', 'Bergrivier Local Municipality', 'Swartland Local Municipality', 'Umvoti Local Municipality', 'Siyancuma Local Municipality', 'Mangaung Metropolitan Municipality', 'uMfolozi Local Municipality', 'Merafong City Local Municipality', 'Newcastle', 'Merafong', 'Dikgatlong Local Municipality', 'Ehlanzeni', 'Raymond Mhlaba Local Municipality', 'uMlalazi Local Municipality', 'Mandeni Local Municipality', 'West Rand District Municipality', 'Setsoto Local Municipality', 'Umhlathuze', 'Lekwa-Teemane Local Municipality', 'Enoch Mgijima Local Municipality', 'Buffalo City Metropolitan Municipality', 'City of Cape Town Metropolitan Municipality', 'City of Johannesburg Metropolitan Municipality', 'Kai\\xa0!Garib Local Municipality', 'Midvaal Local Municipality', 'Nquthu Local Municipality', 'Nelson Mandela bay', 'Lepelle-Nkumpi Local Municipality', 'Merafong Municipality', 'Knysna Local Municipality', 'Msukaligwa Local Municipality', 'Ulundi Local Municipality', 'Maquassi Hills Local Municipality', 'Greater Tzaneen Local Municipality', 'Ulundi', 'Newcastle Local Municipality', 'Greater Taung Local Municipality', 'Okhahlamba Local Municipality', 'Amahlathi Local Municipality', 'Mogale City Local Municipality', 'COJ', 'Mthonjaneni Local Municipality', 'Bushbuckridge Local Municipality', 'Oudtshoorn Local Municipality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:645: FutureWarning: Passing 'suffixes' which cause duplicate columns {'y_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:701: DtypeWarning: Columns (5,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
      "C:\\Users\\LazolaJavu\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py:722: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 500\n",
      "{'Guid': '46b0d437-5e46-ea11-833a-00155d326100', 'FullName': 'Nokwanda\\xa0Phungula ', 'IdNumber': '9110101294088', 'PersonalNumber': '0825839766', 'BirthDate': '1991-10-10', 'CountryOfCitizenship': 'South African', 'Age': '28', 'InactivityComments': 'Unable to reach', 'MonthsSinceFranchisee': 19, 'IsSouthAfricanCitizen': True, 'VerifiedByHomeAffairs': True, 'AttendedChildProgress': False, 'AttendedBusinessSkills': None, 'IsClubLeader': False, 'StartDate': '2020-01-23T22:00:00Z', 'Status': 'Inactive', 'Gender': 'Female', 'ProgrammeType': 'Playgroup', 'EthnicGroup': 'African', 'Province': 'KwaZulu-Natal', 'ReasonForLeaving': 'Franchisee is uncontactable', 'Franchisor': {'Guid': '6e89ad3f-e278-e611-80c7-0050568109d5'}, 'Coach': {'Guid': 'b4e5c772-26ef-e911-8326-0800274bb0e4'}, 'Principal': None, 'SiteAddress': None}\n",
      "{'FullName': 'Lerato\\xa0Lehutso ', 'DateOfBirth': '1986-06-06', 'AreaOfOperation': 'Greater Taung', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6289ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Active', 'Guid': '31cc9b33-0a57-ea11-833a-00155d326100'}\n",
      "{'FullName': 'Amanda Emily\\xa0Mapolisa ', 'DateOfBirth': None, 'AreaOfOperation': 'Orange farm', 'Gender': 'Female', 'EthnicGroup': 'African', 'WorkAddressProvince': None, 'Franchisor': {'Guid': '6a89ad3f-e278-e611-80c7-0050568109d5'}, 'Status': 'Inactive', 'Guid': 'b0ebc959-a507-eb11-8343-00155d326100'}\n",
      "[{\"Name\":\"KZN Branch\",\"Guid\":\"5a6b40ec-1e4f-eb11-8345-00155d326100\"},{\"Name\":\"KET\",\"Guid\":\"3ed99252-d216-ec11-834c-00155d326100\"},{\"Name\":\"Lesedi Solar\",\"Guid\":\"e7b9933d-da35-ed11-8351-00155d326100\"},{\"Name\":\"Letsatsi Solar\",\"Guid\":\"821d436d-df4a-ed11-8355-00155d326100\"},{\"Name\":\"PPTrust\",\"Guid\":\"90b04590-6c96-ed11-8356-00155d326100\"},{\"Name\":\"NW Branch\",\"Guid\":\"6289ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"GP Branch\",\"Guid\":\"6a89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Khululeka\",\"Guid\":\"6c89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"LETCEE\",\"Guid\":\"6e89ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"TREE\",\"Guid\":\"7089ad3f-e278-e611-80c7-0050568109d5\"},{\"Name\":\"Siyakholwa\",\"Guid\":\"4b16de13-e386-e611-80ca-0050568109d5\"},{\"Name\":\"SAYM\",\"Guid\":\"474b1eed-e486-e611-80ca-0050568109d5\"},{\"Name\":\"Diaconia\",\"Guid\":\"cb6f6d1a-e686-e611-80ca-0050568109d5\"},{\"Name\":\"LIMA\",\"Guid\":\"f3e19b66-e786-e611-80ca-0050568109d5\"},{\"Name\":\"ELRU\",\"Guid\":\"20b4261f-e886-e611-80ca-0050568109d5\"},{\"Name\":\"Lesedi Educare Association\",\"Guid\":\"7911a744-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"3L Development\",\"Guid\":\"812aa76c-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Molteno\",\"Guid\":\"9093d385-4584-e811-817d-0800274bb0e4\"},{\"Name\":\"Penreach\",\"Guid\":\"0804f6ac-4584-e811-817d-0800274bb0e4\"}]\n",
      "(6651, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaed5955c944104b14abd5bf0f6087a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LazolaJavu\\anaconda3.1\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n",
      "C:\\Users\\LazolaJavu\\anaconda3.1\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n",
      "C:\\Users\\LazolaJavu\\anaconda3.1\\lib\\site-packages\\ydata_profiling\\model\\correlations.py:67: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/ydata-profiling/issues\n",
      "(include the error message: 'No data; `observed` has size 0.')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbd84ac861f45ee8d401c570ed10ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330ac81d37448fc948a8880a3c000a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4239ea3cbc4de9ae04dfbb4e348318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    290\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    290\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[0mdf_ELRU_NW_inactive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_franchisee_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"6289ad3f-e278-e611-80c7-0050568109d5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[1;31m## 6 GP Branch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m \u001b[0mdf_GP_Branch_inactive\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mget_franchisee_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"6a89ad3f-e278-e611-80c7-0050568109d5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;31m## 7 Khululeka\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[0mdf_Khululeka_inactive\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mget_franchisee_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"6c89ad3f-e278-e611-80c7-0050568109d5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23500\\345988220.py\u001b[0m in \u001b[0;36mget_franchisee_data\u001b[1;34m(franchisor_value)\u001b[0m\n\u001b[0;32m   1220\u001b[0m     }\n\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#################### ACTIVE CHILDREN #######################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "#################### INACTIVE CHILDREN############################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "# Save the data into a csv file.\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/children_data.csv\")\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "# from fbprophet import Prophet\n",
    "from functools import reduce\n",
    "# from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "start_date = datetime.date(2016, 1, 1)  # start date\n",
    "end_date = datetime.date(2031, 12, 19)  # end date\n",
    "num_days = (end_date - start_date).days + 1  # number of days between start and end dates\n",
    "\n",
    "# create array of values\n",
    "values = np.linspace(100, 100500, num_days)  # starting value of x and ending value of x_1\n",
    "# create array of values\n",
    "City_of_Johannesburg = np.linspace(332957, 332957, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Cape_Town = np.linspace(169677, 169677, num_days)  # starting value of x and ending value of x_1\n",
    "Polokwane = np.linspace(72052, 72052, num_days)  # starting value of x and ending value of x_1\n",
    "Ekurhuleni = np.linspace(209777, 209777, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Tshwane = np.linspace(158393, 158393, num_days)  # starting value of x and ending value of x_1\n",
    "Bushbuckridge = np.linspace(40688, 40688, num_days)  # starting value of x and ending value of x_1\n",
    "Ehlanzeni = np.linspace(116582, 116582, num_days)  # starting value of x and ending value of x_1\n",
    "Bojanala = np.linspace(151168, 151168, num_days)  # starting value of x and ending value of x_1\n",
    "Sedibeng = np.linspace(79595, 79595, num_days)  # starting value of x and ending value of x_1\n",
    "Madibeng = np.linspace(54912, 54912, num_days)  # starting value of x and ending value of x_1\n",
    "Collins_Chabane = np.linspace(50667, 50667, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Mbombela = np.linspace(79542, 79542, num_days)  # starting value of x and ending value of x_1\n",
    "Nyandeni = np.linspace(47504, 47504, num_days)  # starting value of x and ending value of x_1\n",
    "Rustenburg = np.linspace(68991, 68991, num_days)  # starting value of x and ending value of x_1\n",
    "Buffalo_City = np.linspace(72685, 72685, num_days)  # starting value of x and ending value of x_1\n",
    "King_Sabata_Dalindyebo = np.linspace(59373, 59373, num_days)  # starting value of x and ending value of x_1\n",
    "Nkomazi = np.linspace(53994, 53994, num_days)  # starting value of x and ending value of x_1\n",
    "Emfuleni = np.linspace(62590, 62590, num_days)  # starting value of x and ending value of x_1\n",
    "Nelson_Mandela_Bay = np.linspace(65905, 65905, num_days)  # starting value of x and ending value of x_1\n",
    "Thembisile = np.linspace(50006, 50006, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Matlosana = np.linspace(45887, 45887, num_days)  # starting value of x and ending value of x_1\n",
    "\n",
    "\n",
    "# create array of dates\n",
    "dates = [start_date + datetime.timedelta(days=i) for i in range(num_days)]\n",
    "\n",
    "# create dataframe\n",
    "data_muni_gap = pd.DataFrame({'date': dates, 'value': values, 'City of Johannesburg':City_of_Johannesburg,\n",
    "                   'City of Cape Town':City_of_Cape_Town, 'Polokwane':Polokwane, 'Ekurhuleni':Ekurhuleni,\n",
    "                   'City of Tshwane':City_of_Tshwane,'Bushbuckridge':Bushbuckridge,'Ehlanzeni':Ehlanzeni,'Bojanala':Bojanala,\n",
    "                   'Sedibeng':Sedibeng,'Local Municipality of Madibeng':Madibeng,'Collins Chabane':Collins_Chabane,'City of Mbombela':City_of_Mbombela,\n",
    "                   'Nyandeni':Nyandeni,'Rustenburg':Rustenburg,'Buffalo City':Buffalo_City,'King Sabata Dalindyebo':King_Sabata_Dalindyebo,\n",
    "                   'Nkomazi':Nkomazi,'Emfuleni':Emfuleni,'Nelson Mandela Bay':Nelson_Mandela_Bay,'Thembisile':Thembisile, 'City of Matlosana':City_of_Matlosana})\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "data = data[['Guid', 'FullName', 'StartDate', 'InactiveDate', 'Status',\n",
    "       'Franchisee.FullName', 'Franchisee.IdNumber', 'Franchisee.Province',\n",
    "       'Franchisee.ProgrammeType', 'Franchisee.Guid']]\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "data_children = data.copy()\n",
    "\n",
    "# read_csv file:\n",
    "site_municipalities = pd.read_excel(r\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/Site Municipality.xlsx\")\n",
    "\n",
    "# read_csv file:\n",
    "data_franchisee = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "data_franchisee =data_franchisee[['Guid', 'FullName', 'IdNumber',\n",
    "       'Gender', 'ProgrammeType', 'EthnicGroup', 'Province',\n",
    "       'ReasonForLeaving', 'Principal', 'SiteAddress', 'Franchisor.Guid',\n",
    "       'Coach.Guid', 'SiteAddress.Municipality',\n",
    "       'Franchisor']]\n",
    "# # Merge the excel file with the data we pulled from\n",
    "merge_df = pd.merge(data_children, data_franchisee, left_on = 'Franchisee.IdNumber', right_on = 'IdNumber')\n",
    "merge_df = pd.merge(merge_df, site_municipalities, left_on = 'Franchisee.IdNumber', right_on = 'ID Number')\n",
    "# Data copy\n",
    "data = merge_df.copy()\n",
    "# select the franchisees that are active.\n",
    "data_cpt_active = data[data['Status'] == 'Active']\n",
    "# Use the province column to check the model performance by Province\n",
    "data_cpt_muni = data_cpt_active[['StartDate','Site Municipality']]\n",
    "# Now let us count the data of the site municipality\n",
    "data_cpt_muni['Site Municipality'].value_counts().to_frame()\n",
    "\n",
    "# Drop NaN values from the 'Site Municipality' column\n",
    "data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n",
    "\n",
    "# Get unique values from the 'Site Municipality' column\n",
    "unique_values = list(set(data_cpt_active['Site Municipality']))\n",
    "\n",
    "print(unique_values)\n",
    "\n",
    "municipality_list = unique_values\n",
    "\n",
    "data_cpt_muni = data_cpt_muni.loc[data['Site Municipality'].isin(municipality_list)]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_cpt_muni['Site Municipality'])\n",
    "# merge the datasets\n",
    "data_cpt_muni = data_cpt_muni.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_cpt_muni['id'] = data_cpt_muni['StartDate']\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['StartDate']\n",
    "# Reset the index\n",
    "data_cpt_muni.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['Site Municipality']\n",
    "# View the columns\n",
    "municipality_list_2 = list(data_cpt_muni.columns)\n",
    "# Create a copy of the data\n",
    "df_grouped = data_cpt_muni.copy()\n",
    "# Rearrange the columns from the dataframe\n",
    "df_grouped = df_grouped[municipality_list_2]\n",
    "df_grouped.reset_index()\n",
    "# convert 'date' column to date format\n",
    "df_grouped['id'] = pd.to_datetime(df_grouped['id'], errors='coerce')\n",
    "\n",
    "# Convert start_date to datetime object\n",
    "start_date = datetime.datetime.combine(datetime.date(2016, 1, 1), datetime.datetime.min.time())\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "\n",
    "# Filter the DataFrame based on the date range\n",
    "df_grouped = df_grouped[(df_grouped['id'] >= start_date) & (df_grouped['id'] <= end_date)]\n",
    "# Define function to create time series\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')[['id', column_name]]\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    data = data.rename(columns={'id': 'ds', column_name: 'y'})\n",
    "    data = data.dropna().drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "# Apply function to create time series for selected cities\n",
    "cities = municipality_list\n",
    "df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
    "\n",
    "# Rename the columns\n",
    "df_grouped_basic.columns = ['ds'] + municipality_list\n",
    "\n",
    "\n",
    "df_grouped_basic.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/df_grouped_basic.csv\")\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "# import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from streamlit import components\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import savgol_filter\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import folium\n",
    "from streamlit_folium import folium_static\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Libraries needed for the tutorial\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Username of your GitHub account\n",
    "\n",
    "username = 'lazola.javu@gmail.com'\n",
    "\n",
    "# Personal Access Token (PAO) from your GitHub account\n",
    "\n",
    "# token = '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'\n",
    "\n",
    "token = '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'\n",
    "\n",
    "########### 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 #################### Token doesn't expire\n",
    "\n",
    "# Creates a re-usable session object with your creds in-built\n",
    "github_session = requests.Session()\n",
    "github_session.auth = (username, token)\n",
    "\n",
    "url_4 = \"https://raw.githubusercontent.com/LazolaJavu/Time-Series-Forecasting/main/Data/external/ECD_Data.csv?token=GHSAT0AAAAAACBIG46DYRXUHQIDWHOEL6B6ZEQEMQA\"\n",
    "ecd_census_data = github_session.get(url_4).content\n",
    "\n",
    "# Reading the downloaded content and making it a pandas dataframe\n",
    "ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
    "# Read the shape file\n",
    "shape_file = gpd.read_file(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\")\n",
    "# Read the shape file\n",
    "gdf = shape_file[shape_file['geonunit'] == 'South Africa']\n",
    "# Read the children data\n",
    "df = pd.read_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv')\n",
    "# Rename the column table\n",
    "df['Province'] = df['Franchisee.Province']\n",
    "# Group the data by province and count the values\n",
    "province_counts = df['Province'].value_counts().reset_index()\n",
    "province_counts.columns = ['Province', 'Counts']\n",
    "merged_data = gdf.merge(province_counts, left_on='woe_name', right_on='Province', how='left')\n",
    "# Copy the merged data\n",
    "gdf = merged_data.copy()\n",
    "# Save a csv file:\n",
    "gdf.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/gdf.csv\")\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "print(data.shape)\n",
    "# Create the Profile Report\n",
    "profile = ProfileReport(data, title = \"SmartStart Franchisees EDA\")\n",
    "# Save as an html file\n",
    "profile.to_file(\"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/franchisee_report.html\")\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "################################### GET CHILDREN DATA #######################################\n",
    "def get_franchisee_data(franchisor_value):\n",
    "\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\"Guid\",\n",
    "    \"FullName\",\n",
    "    \"FirstName\",\n",
    "    \"Surname\",\n",
    "    \"IdNumber\",\n",
    "    \"AllergyType\",\n",
    "    \"DisabilityType\",\n",
    "    \"HealthConditions\",\n",
    "    \"EmergencyContactNumber\",\n",
    "    \"EmergencyContactFullName\",\n",
    "    \"EmergencyContactFirstName\",\n",
    "    \"EmergencyContactSurname\",\n",
    "    \"AlternativePickupFirstName\",\n",
    "    \"AlternativePickupSurname\",\n",
    "    \"AlternativePickupContactNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"StartDate\",\n",
    "    \"HasAllergy\",\n",
    "    \"HasDisability\",\n",
    "    \"CaregiverPopiaConsent\",\n",
    "    \"CaregiverPhotographyAndFilmingConsent\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"HasIdNumber\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"HomeLanguage\",\n",
    "    \"GrantType\",\n",
    "    \"PlaygroupGroup\",\n",
    "    \"InactiveReason\",\n",
    "    \"Franchisee\",\n",
    "    \"Caregiver\",\n",
    "    \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\"Guid\",\n",
    "    \"FullName\",\n",
    "    \"FirstName\",\n",
    "    \"Surname\",\n",
    "    \"IdNumber\",\n",
    "    \"AllergyType\",\n",
    "    \"DisabilityType\",\n",
    "    \"HealthConditions\",\n",
    "    \"EmergencyContactNumber\",\n",
    "    \"EmergencyContactFullName\",\n",
    "    \"EmergencyContactFirstName\",\n",
    "    \"EmergencyContactSurname\",\n",
    "    \"AlternativePickupFirstName\",\n",
    "    \"AlternativePickupSurname\",\n",
    "    \"AlternativePickupContactNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"StartDate\",\n",
    "    \"HasAllergy\",\n",
    "    \"HasDisability\",\n",
    "    \"CaregiverPopiaConsent\",\n",
    "    \"CaregiverPhotographyAndFilmingConsent\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"HasIdNumber\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"HomeLanguage\",\n",
    "    \"GrantType\",\n",
    "    \"PlaygroupGroup\",\n",
    "    \"InactiveReason\",\n",
    "    \"Franchisee\",\n",
    "    \"Caregiver\",\n",
    "    \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "\n",
    "profile = ProfileReport(data, minimal=True)\n",
    "profile.to_file(output_file=\"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/Children_report_report.html\")\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "data_mod = data_mod[data_mod['id'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroup.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstart.csv')\n",
    "\n",
    "##################################################################### Franchisee #############################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data_mod = data_mod[data_mod['id'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data_mod['id'] = data_mod['id'].dt.strftime('%Y-%m-%d')\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd_franchisees.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother_franchisees.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroupdata_franchisees.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstartdata_franchisees.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Franchisee.Province' ]]\n",
    "df_sd = data[['StartDate', 'Franchisee.Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Franchisee.Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Franchisee.Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Franchisee.Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Franchisee.Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS.csv')\n",
    "\n",
    "############################################################################## FRANCHISEES #################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data = data[data['StartDate'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data['StartDate'] = data['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Province' ]]\n",
    "df_sd = data[['StartDate', 'Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS_franchisees.csv')\n",
    "\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "########################################################################### CHILDREN DATA ##########################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_children_data.csv')\n",
    "\n",
    "################################################################# franchisees Data ##########################################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Convert the 'StartDate' column to datetime format\n",
    "df_dates['StartDate'] = pd.to_datetime(df_dates['StartDate'], errors='coerce')\n",
    "\n",
    "# Extract only the date portion from the 'StartDate' column\n",
    "df_dates['StartDate'] = df_dates['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_Franchisee_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586dfbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T19:43:27.412821Z",
     "start_time": "2023-08-02T19:40:54.839908Z"
    }
   },
   "outputs": [],
   "source": [
    "################################################ SMARTSTART GHS DATA ################################################\n",
    "############################################### FRANCHISEE GHS DATA #################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_date(begin_date, finish_date, sv, ev):\n",
    "    # Define the start and end dates\n",
    "    start_date = pd.to_datetime(begin_date, format=\"%d-%m-%Y\")\n",
    "    end_date = pd.to_datetime(finish_date, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Define the start and end values\n",
    "    start_value = sv\n",
    "    end_value = ev\n",
    "\n",
    "    # Create a range of dates\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Generate random increasing values\n",
    "    num_values = len(dates)\n",
    "    random_values = np.linspace(start_value, end_value, num_values)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Date': dates, 'Value': random_values})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the individual dataframes\n",
    "df1 = create_date('01-07-2021', '30-06-2022', 1033, 764)\n",
    "df2 = create_date('01-07-2022', '30-06-2023', 764, 1440)\n",
    "df3 = create_date('01-07-2023', '30-06-2024', 1440, 1842)\n",
    "df4 = create_date('01-07-2024', '30-06-2025', 1842, 17979)\n",
    "df5 = create_date('01-07-2025', '30-06-2026', 17979, 24290)\n",
    "df6 = create_date('01-07-2026', '30-06-2027', 24290, 31476)\n",
    "df7 = create_date('01-07-2027', '30-06-2028', 31476, 38851)\n",
    "df8 = create_date('01-07-2028', '30-06-2029', 38851, 42326)\n",
    "df9 = create_date('01-07-2029', '30-06-2030', 42326, 41299)\n",
    "df10 = create_date('01-07-2030', '30-06-2031', 41299, 42955)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_franchisees = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_franchisees.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_franchisees)\n",
    "\n",
    "df_GHS_franchisees.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_franchisees_Modality.csv\")\n",
    "########################################## CHILDREN GHS DATA #####################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_date(begin_date, finish_date, sv, ev):\n",
    "    # Define the start and end dates\n",
    "    start_date = pd.to_datetime(begin_date, format=\"%d-%m-%Y\")\n",
    "    end_date = pd.to_datetime(finish_date, format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Define the start and end values\n",
    "    start_value = sv\n",
    "    end_value = ev\n",
    "\n",
    "    # Create a range of dates\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Generate random increasing values\n",
    "    num_values = len(dates)\n",
    "    random_values = np.linspace(start_value, end_value, num_values)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Date': dates, 'Value': random_values})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create the individual dataframes\n",
    "df1 = create_date('01-07-2021', '30-06-2022', 3500, 4722)\n",
    "df2 = create_date('01-07-2022', '30-06-2023', 4722, 8219)\n",
    "df3 = create_date('01-07-2023', '30-06-2024', 8219, 11149)\n",
    "df4 = create_date('01-07-2024', '30-06-2025', 11149, 17979)\n",
    "df5 = create_date('01-07-2025', '30-06-2026', 17979, 24290)\n",
    "df6 = create_date('01-07-2026', '30-06-2027', 24290, 31476)\n",
    "df7 = create_date('01-07-2027', '30-06-2028', 31476, 38851)\n",
    "df8 = create_date('01-07-2028', '30-06-2029', 38851, 42326)\n",
    "df9 = create_date('01-07-2029', '30-06-2030', 42326, 41299)\n",
    "df10 = create_date('01-07-2030', '30-06-2031', 41299, 42955)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_franchisees = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_franchisees.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_franchisees)\n",
    "\n",
    "df_GHS_franchisees.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_franchisees.csv\")\n",
    "\n",
    "df1 = create_date('01-07-2021','30-06-2022',45000,51824)\n",
    "df2 = create_date('01-07-2022','30-06-2023',51824,76800)\n",
    "df3 = create_date('01-07-2023','30-06-2024',76800,130000)\n",
    "df4 = create_date('01-07-2024','30-06-2025',130000,170000)\n",
    "df5 = create_date('01-07-2025','30-06-2026',170000,294000)\n",
    "df6 = create_date('01-07-2026','30-06-2027',294000,392000)\n",
    "df7 = create_date('01-07-2027','30-06-2028',392000,561000)\n",
    "df8 = create_date('01-07-2028','30-06-2029',561000,612000)\n",
    "df9 = create_date('01-07-2029','30-06-2030',612000,600000)\n",
    "df10 = create_date('01-07-2030','30-06-2031',600000,600000)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_GHS_children = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df_GHS_children.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(df_GHS_children)\n",
    "\n",
    "df_GHS_children.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/df_GHS_children.csv\")\n",
    "################################ Download Children DATA ########################################\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#################### ACTIVE CHILDREN #######################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Active\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "#################### INACTIVE CHILDREN############################################\n",
    "\n",
    "def get_franchisee_data(franchisor_value):\n",
    "    url = \"https://api.smartstart.org.za/v1/Child/Query/\"\n",
    "    payload = json.dumps({\n",
    "      \"Columns\": [\n",
    "        \"Guid\",\n",
    "        \"FullName\",\n",
    "        \"StartDate\",\n",
    "        \"InactiveDate\",\n",
    "        \"Status\"\n",
    "      ],\n",
    "      \"Conditions\": [\n",
    "        {\n",
    "          \"Column\": \"Status\",\n",
    "          \"Operator\": \"Equals\",\n",
    "          \"Value\": \"Inactive\"\n",
    "        }\n",
    "      ],\n",
    "      \"Related\": [\n",
    "        {\n",
    "          \"RelatedBy\": \"Franchisee\",\n",
    "          \"Columns\": [\n",
    "            \"FullName\",\n",
    "            \"IdNumber\",\n",
    "            \"Province\",\n",
    "            \"ProgrammeType\"\n",
    "          ],\n",
    "          \"Conditions\": [\n",
    "            {\n",
    "              \"Column\": \"Franchisor\",\n",
    "              \"Operator\": \"Equals\",\n",
    "              \"Value\": franchisor_value\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "## 1. KZN Franchisor\n",
    "df_kzn_inactive = get_franchisee_data(\"5a6b40ec-1e4f-eb11-8345-00155d326100\")\n",
    "## 2. KZN Franchisor\n",
    "df_KET_inactive = get_franchisee_data(\"3ed99252-d216-ec11-834c-00155d326100\")\n",
    "## 3. Lesedi_Solar\n",
    "df_Lesedi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 4. Letsatsi Solar\n",
    "df_Letsatsi_Solar_inactive = get_franchisee_data(\"e7b9933d-da35-ed11-8351-00155d326100\")\n",
    "## 5. PPTrust\n",
    "df_PPTrust_inactive = get_franchisee_data(\"90b04590-6c96-ed11-8356-00155d326100\")\n",
    "## 6. ELRU-NW\n",
    "df_ELRU_NW_inactive = get_franchisee_data(\"6289ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 6 GP Branch\n",
    "df_GP_Branch_inactive  = get_franchisee_data(\"6a89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 7 Khululeka\n",
    "df_Khululeka_inactive  = get_franchisee_data(\"6c89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 8 LETCEE\n",
    "df_LETCEE_inactive  = get_franchisee_data(\"6e89ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 9 TREE\n",
    "df_TREE_inactive  = get_franchisee_data(\"7089ad3f-e278-e611-80c7-0050568109d5\")\n",
    "## 10 Siyakholwa\n",
    "df_Siyakholwa_inactive\t=\tget_franchisee_data('4b16de13-e386-e611-80ca-0050568109d5')\n",
    "## 11 SAYM\n",
    "df_SAYM_inactive\t=\tget_franchisee_data('474b1eed-e486-e611-80ca-0050568109d5')\n",
    "## 12 Diaconia\n",
    "df_Diaconia_inactive\t=\tget_franchisee_data('cb6f6d1a-e686-e611-80ca-0050568109d5')\n",
    "## 13 LIMA\n",
    "df_LIMA_inactive\t=\tget_franchisee_data('f3e19b66-e786-e611-80ca-0050568109d5')\n",
    "## 14 ELRU\n",
    "df_ELRU_inactive\t=\tget_franchisee_data('20b4261f-e886-e611-80ca-0050568109d5')\n",
    "## 15 Lesedi_Educare_Association\n",
    "df_Lesedi_Educare_Association_inactive\t=\tget_franchisee_data('7911a744-4584-e811-817d-0800274bb0e4')\n",
    "## 16 3L_Development\n",
    "df_3L_Development_inactive\t=\tget_franchisee_data('812aa76c-4584-e811-817d-0800274bb0e4')\n",
    "## 17 df_Molteno\n",
    "df_Molteno_inactive\t=\tget_franchisee_data('9093d385-4584-e811-817d-0800274bb0e4')\n",
    "## 18 df_Penreach\n",
    "df_Penreach_inactive\t=\tget_franchisee_data('0804f6ac-4584-e811-817d-0800274bb0e4')\n",
    "\n",
    "# Show the dataframes\n",
    "frames = [df_kzn, df_KET, df_Lesedi_Solar, df_Letsatsi_Solar, df_PPTrust, df_ELRU_NW, df_GP_Branch, df_Khululeka, df_LETCEE, df_TREE, df_Siyakholwa, df_SAYM, df_Diaconia, df_LIMA, df_ELRU, df_Lesedi_Educare_Association, df_3L_Development, df_Molteno, df_Penreach, df_kzn_inactive, df_KET_inactive, df_Lesedi_Solar_inactive, df_Letsatsi_Solar_inactive, df_PPTrust_inactive, df_ELRU_NW_inactive, df_GP_Branch_inactive, df_Khululeka_inactive, df_TREE_inactive, df_Siyakholwa_inactive, df_SAYM_inactive, df_Diaconia_inactive, df_LIMA_inactive, df_ELRU_inactive, df_Lesedi_Educare_Association_inactive, df_3L_Development_inactive, df_Molteno_inactive, df_Penreach_inactive]\n",
    "# Concatenate the dataframes\n",
    "data = pd.concat(frames)\n",
    "# Save the data into a csv file.\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/children_data.csv\")\n",
    "data.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "######################################################################## Read shapefile################################\n",
    "#set up the file path and read the shapefile data\n",
    "fp = \"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Live Data Connection/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shx\"\n",
    "map_df = gpd.read_file(fp)\n",
    "# map_df.to_crs(pyproj.CRS.from_epsg(4326), inplace=True)\n",
    "map_df = map_df[map_df['admin']=='South Africa']\n",
    "map_df = map_df[['name','adm1_code', 'diss_me','geometry']]\n",
    "\n",
    "merged = map_df.copy()\n",
    "merged\n",
    "data = data[data['Status'] == 'Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de324694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "value_counts_df = data['Province'].value_counts().reset_index()\n",
    "print(value_counts_df)\n",
    "merged = pd.merge(merged, value_counts_df, how = 'left',left_on = 'name', right_on='index')\n",
    "# Assuming you have a GeoDataFrame named 'df' that you want to save as GeoJSON\n",
    "output_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map_children.geojson\"\n",
    "# Save the GeoDataFrame as GeoJSON\n",
    "merged.to_file(output_fp, driver='GeoJSON')\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "input_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map_children.geojson\"\n",
    "gdf = gpd.read_file(input_fp)\n",
    "\n",
    "#################################################### DOWNLOAD FRANCHISEE DATA #########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt #if using matplotlib\n",
    "import plotly.express as px #if using plotly\n",
    "import geopandas as gpd\n",
    "################################### Active FRANCHISEES ####################\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_active = pd.json_normalize(json_data)\n",
    "\n",
    "################################### INACTIVE FRANCHISEES ####################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisee/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Guid\",\n",
    "    \"FullName\",\n",
    "    \"IdNumber\",\n",
    "    \"PersonalNumber\",\n",
    "    \"BirthDate\",\n",
    "    \"CountryOfCitizenship\",\n",
    "    \"Age\",\n",
    "    \"InactivityComments\",\n",
    "    \"MonthsSinceFranchisee\",\n",
    "    \"IsSouthAfricanCitizen\",\n",
    "    \"VerifiedByHomeAffairs\",\n",
    "    \"AttendedChildProgress\",\n",
    "    \"AttendedBusinessSkills\",\n",
    "    \"IsClubLeader\",\n",
    "    \"StartDate\",\n",
    "    \"InactiveDate\",\n",
    "    \"Status\",\n",
    "    \"Gender\",\n",
    "    \"ProgrammeType\",\n",
    "    \"EthnicGroup\",\n",
    "    \"Province\",\n",
    "    \"ReasonForLeaving\",\n",
    "    \"Franchisor\",\n",
    "    \"Coach\",\n",
    "    \"Principal\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ],\n",
    "  \"Related\": [\n",
    "    {\n",
    "      \"RelatedBy\": \"SiteAddress\",\n",
    "      \"JoinType\": \"Outer\",\n",
    "      \"Columns\": [\n",
    "        \"Municipality\",\n",
    "        \"Name\",\n",
    "        \"PostalCode\",\n",
    "        \"Ward\",\n",
    "        \"Area\",\n",
    "        \"StreetAddress\",\n",
    "        \"SharedFullAddress\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"SharedLatitude\",\n",
    "        \"SharedLongitude\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_inactive = pd.json_normalize(json_data)\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "\n",
    "###################### COACH DATA ACTIVE##################################\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach = pd.json_normalize(json_data)\n",
    "\n",
    "############################# INACTIVE COACHES ##########################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Coach/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"FullName\",\n",
    "    \"DateOfBirth\",\n",
    "    \"AreaOfOperation\",\n",
    "    \"Gender\",\n",
    "    \"EthnicGroup\",\n",
    "    \"WorkAddressProvince\",\n",
    "    \"Franchisor\",\n",
    "    \"Status\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Inactive\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Print only the first franchisees\n",
    "    print(data[1])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_coach_inactive = pd.json_normalize(json_data)\n",
    "\n",
    "###################################### ACTIVE FRANCHISOR ##################\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.smartstart.org.za/v1/Franchisor/Query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"Columns\": [\n",
    "    \"Name\"\n",
    "  ],\n",
    "  \"Conditions\": [\n",
    "    {\n",
    "      \"Column\": \"Status\",\n",
    "      \"Operator\": \"Equals\",\n",
    "      \"Value\": \"Active\"\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'API-Key': '4369C387-DAB7-418A-A623-00B91753D0C3'\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Assuming the JSON data is stored in text\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON data to a dataframe\n",
    "df_franchisor = pd.json_normalize(json_data)\n",
    "\n",
    "########################## CONCATENATE THE DATAFRAMES #################\n",
    "\n",
    "# Collect the dataframes into a list that will be concatenated\n",
    "frames_franchisees = [df_active, df_inactive]\n",
    "frames_coach = [df_coach, df_coach_inactive]\n",
    "# Create a concat the dataframes\n",
    "df_franchisees = pd.concat(frames_franchisees)\n",
    "df_coaches = pd.concat(frames_coach)\n",
    "\n",
    "df1 = df_franchisees.copy()\n",
    "df2 = df_coaches.copy()\n",
    "df3 = df_franchisor.copy()\n",
    "\n",
    "# merge the two DataFrames on 'guid' and 'franchisor.guid'\n",
    "merged_df = pd.merge(df2, df3, left_on='Franchisor.Guid', right_on='Guid')\n",
    "# Remove the columns 'Franchisor.Guid' and 'Guid_y'\n",
    "merged_df = merged_df.drop(labels = ['Franchisor.Guid', 'Guid_y'], axis = 1)\n",
    "merged_df.columns = ['Coach', 'DateOfBirth_Coach', 'AreaOfOperation_coach', 'Gender_coach', 'EthnicGroup_coach',\n",
    "       'WorkAddressProvince_coach', 'Status_coach', 'Coach.Guid', 'Franchisor']\n",
    "# merge the datasets\n",
    "data = pd.merge(df1, merged_df, left_on='Coach.Guid', right_on='Coach.Guid')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv')\n",
    "\n",
    "#set up the file path and read the shapefile data\n",
    "fp = \"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Live Data Connection/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shx\"\n",
    "map_df = gpd.read_file(fp)\n",
    "# map_df.to_crs(pyproj.CRS.from_epsg(4326), inplace=True)\n",
    "map_df = map_df[map_df['admin']=='South Africa']\n",
    "map_df = map_df[['name','adm1_code', 'diss_me','geometry']]\n",
    "\n",
    "merged = map_df.copy()\n",
    "merged\n",
    "data = data[data['Status'] == 'Active']\n",
    "value_counts_df = data['Province'].value_counts().reset_index()\n",
    "value_counts_df\n",
    "merged = pd.merge(merged, value_counts_df, how = 'left',left_on = 'name', right_on='index')\n",
    "# Assuming you have a GeoDataFrame named 'df' that you want to save as GeoJSON\n",
    "output_fp = \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map.geojson\"\n",
    "# Save the GeoDataFrame as GeoJSON\n",
    "merged.to_file(output_fp, driver='GeoJSON')\n",
    "# Read the GeoJSON file into a GeoDataFrame\n",
    "input_fp =  \"C:/Users/LazolaJavu/OneDrive - SmartStart/Desktop/Data-Science-Notes/data/provinces_to_map.geojson\"\n",
    "gdf = gpd.read_file(input_fp)\n",
    "\n",
    "############################################### CHILDREN BY MUNICIPALITY ###################################\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "# from fbprophet import Prophet\n",
    "from functools import reduce\n",
    "# from fbprophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "start_date = datetime.date(2016, 1, 1)  # start date\n",
    "end_date = datetime.date(2031, 12, 19)  # end date\n",
    "num_days = (end_date - start_date).days + 1  # number of days between start and end dates\n",
    "\n",
    "# create array of values\n",
    "values = np.linspace(100, 100500, num_days)  # starting value of x and ending value of x_1\n",
    "# create array of values\n",
    "City_of_Johannesburg = np.linspace(332957, 332957, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Cape_Town = np.linspace(169677, 169677, num_days)  # starting value of x and ending value of x_1\n",
    "Polokwane = np.linspace(72052, 72052, num_days)  # starting value of x and ending value of x_1\n",
    "Ekurhuleni = np.linspace(209777, 209777, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Tshwane = np.linspace(158393, 158393, num_days)  # starting value of x and ending value of x_1\n",
    "Bushbuckridge = np.linspace(40688, 40688, num_days)  # starting value of x and ending value of x_1\n",
    "Ehlanzeni = np.linspace(116582, 116582, num_days)  # starting value of x and ending value of x_1\n",
    "Bojanala = np.linspace(151168, 151168, num_days)  # starting value of x and ending value of x_1\n",
    "Sedibeng = np.linspace(79595, 79595, num_days)  # starting value of x and ending value of x_1\n",
    "Madibeng = np.linspace(54912, 54912, num_days)  # starting value of x and ending value of x_1\n",
    "Collins_Chabane = np.linspace(50667, 50667, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Mbombela = np.linspace(79542, 79542, num_days)  # starting value of x and ending value of x_1\n",
    "Nyandeni = np.linspace(47504, 47504, num_days)  # starting value of x and ending value of x_1\n",
    "Rustenburg = np.linspace(68991, 68991, num_days)  # starting value of x and ending value of x_1\n",
    "Buffalo_City = np.linspace(72685, 72685, num_days)  # starting value of x and ending value of x_1\n",
    "King_Sabata_Dalindyebo = np.linspace(59373, 59373, num_days)  # starting value of x and ending value of x_1\n",
    "Nkomazi = np.linspace(53994, 53994, num_days)  # starting value of x and ending value of x_1\n",
    "Emfuleni = np.linspace(62590, 62590, num_days)  # starting value of x and ending value of x_1\n",
    "Nelson_Mandela_Bay = np.linspace(65905, 65905, num_days)  # starting value of x and ending value of x_1\n",
    "Thembisile = np.linspace(50006, 50006, num_days)  # starting value of x and ending value of x_1\n",
    "City_of_Matlosana = np.linspace(45887, 45887, num_days)  # starting value of x and ending value of x_1\n",
    "\n",
    "\n",
    "# create array of dates\n",
    "dates = [start_date + datetime.timedelta(days=i) for i in range(num_days)]\n",
    "\n",
    "# create dataframe\n",
    "data_muni_gap = pd.DataFrame({'date': dates, 'value': values, 'City of Johannesburg':City_of_Johannesburg,\n",
    "                   'City of Cape Town':City_of_Cape_Town, 'Polokwane':Polokwane, 'Ekurhuleni':Ekurhuleni,\n",
    "                   'City of Tshwane':City_of_Tshwane,'Bushbuckridge':Bushbuckridge,'Ehlanzeni':Ehlanzeni,'Bojanala':Bojanala,\n",
    "                   'Sedibeng':Sedibeng,'Local Municipality of Madibeng':Madibeng,'Collins Chabane':Collins_Chabane,'City of Mbombela':City_of_Mbombela,\n",
    "                   'Nyandeni':Nyandeni,'Rustenburg':Rustenburg,'Buffalo City':Buffalo_City,'King Sabata Dalindyebo':King_Sabata_Dalindyebo,\n",
    "                   'Nkomazi':Nkomazi,'Emfuleni':Emfuleni,'Nelson Mandela Bay':Nelson_Mandela_Bay,'Thembisile':Thembisile, 'City of Matlosana':City_of_Matlosana})\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "data = data[['Guid', 'FullName', 'StartDate', 'InactiveDate', 'Status',\n",
    "       'Franchisee.FullName', 'Franchisee.IdNumber', 'Franchisee.Province',\n",
    "       'Franchisee.ProgrammeType', 'Franchisee.Guid']]\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "data_children = data.copy()\n",
    "\n",
    "# read_csv file:\n",
    "site_municipalities = pd.read_excel(r\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/Site Municipality.xlsx\")\n",
    "\n",
    "# read_csv file:\n",
    "data_franchisee = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "data_franchisee =data_franchisee[['Guid', 'FullName', 'IdNumber',\n",
    "       'Gender', 'ProgrammeType', 'EthnicGroup', 'Province',\n",
    "       'ReasonForLeaving', 'Principal', 'SiteAddress', 'Franchisor.Guid',\n",
    "       'Coach.Guid', 'SiteAddress.Municipality',\n",
    "       'Franchisor']]\n",
    "# # Merge the excel file with the data we pulled from\n",
    "merge_df = pd.merge(data_children, data_franchisee, left_on = 'Franchisee.IdNumber', right_on = 'IdNumber')\n",
    "merge_df = pd.merge(merge_df, site_municipalities, left_on = 'Franchisee.IdNumber', right_on = 'ID Number')\n",
    "# Data copy\n",
    "data = merge_df.copy()\n",
    "# select the franchisees that are active.\n",
    "data_cpt_active = data[data['Status'] == 'Active']\n",
    "# Use the province column to check the model performance by Province\n",
    "data_cpt_muni = data_cpt_active[['StartDate','Site Municipality']]\n",
    "# Now let us count the data of the site municipality\n",
    "data_cpt_muni['Site Municipality'].value_counts().to_frame()\n",
    "\n",
    "# Drop NaN values from the 'Site Municipality' column\n",
    "data_cpt_active.dropna(subset=['Site Municipality'], inplace=True)\n",
    "\n",
    "# Get unique values from the 'Site Municipality' column\n",
    "unique_values = list(set(data_cpt_active['Site Municipality']))\n",
    "\n",
    "print(unique_values)\n",
    "\n",
    "municipality_list = unique_values\n",
    "\n",
    "data_cpt_muni = data_cpt_muni.loc[data['Site Municipality'].isin(municipality_list)]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_cpt_muni['Site Municipality'])\n",
    "# merge the datasets\n",
    "data_cpt_muni = data_cpt_muni.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_cpt_muni['id'] = data_cpt_muni['StartDate']\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['StartDate']\n",
    "# Reset the index\n",
    "data_cpt_muni.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_cpt_muni['Site Municipality']\n",
    "# View the columns\n",
    "municipality_list_2 = list(data_cpt_muni.columns)\n",
    "# Create a copy of the data\n",
    "df_grouped = data_cpt_muni.copy()\n",
    "# Rearrange the columns from the dataframe\n",
    "df_grouped = df_grouped[municipality_list_2]\n",
    "df_grouped.reset_index()\n",
    "# convert 'date' column to date format\n",
    "df_grouped['id'] = pd.to_datetime(df_grouped['id'], errors='coerce')\n",
    "\n",
    "# Convert start_date to datetime object\n",
    "start_date = datetime.datetime.combine(datetime.date(2016, 1, 1), datetime.datetime.min.time())\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "\n",
    "# Filter the DataFrame based on the date range\n",
    "df_grouped = df_grouped[(df_grouped['id'] >= start_date) & (df_grouped['id'] <= end_date)]\n",
    "# Define function to create time series\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')[['id', column_name]]\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    data = data.rename(columns={'id': 'ds', column_name: 'y'})\n",
    "    data = data.dropna().drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "# Apply function to create time series for selected cities\n",
    "cities = municipality_list\n",
    "df_grouped_basic = reduce(lambda left,right: pd.merge(left,right,on=['ds'], how='outer'), [create_time_series(data_cpt_muni, c) for c in cities])\n",
    "\n",
    "# Rename the columns\n",
    "df_grouped_basic.columns = ['ds'] + municipality_list\n",
    "\n",
    "\n",
    "df_grouped_basic.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/df_grouped_basic.csv\")\n",
    "\n",
    "######################################### ECD CENSUS DATA CONVERSION ###################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ECD_Data.xlsx')\n",
    "\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ECD_Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ FEATURE ENGINEERING - OVERALL ##############################\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "########################################################################### CHILDREN DATA ##########################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_children_data.csv')\n",
    "\n",
    "################################################################# franchisees Data ##########################################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "df_startdate = data.groupby(['StartDate']).size().reset_index(name='counts')\n",
    "# Dataframe with inactive franchisees\n",
    "df_inactivedate = data.groupby(['InactiveDate']).size().reset_index(name='counts')\n",
    "# Assign a negative variable inside a column\n",
    "df_inactivedate['counts'] *= -1\n",
    "# Add a column to the existing df\n",
    "df_inactivedate['StartDate'] = df_inactivedate['InactiveDate']\n",
    "# Delete inactive date\n",
    "del df_inactivedate['InactiveDate']\n",
    "# Change the way the dataframe looks like.\n",
    "df_inactivedate = df_inactivedate[['StartDate', 'counts']]\n",
    "# Add the two dataframes\n",
    "df_dates = pd.concat([df_startdate, df_inactivedate], axis=0)\n",
    "# Sort the values by the date of occurance\n",
    "df_dates = df_dates.sort_values(by='StartDate')\n",
    "# There is an issue with the code dataframe above.\n",
    "# It shows the occurence per day, we want accumulated data.\n",
    "# The cumsum() function computes the values based on the previous one.\n",
    "df_dates['counts'] = df_dates['counts'].cumsum()\n",
    "# Convert the 'StartDate' column to datetime format\n",
    "df_dates['StartDate'] = pd.to_datetime(df_dates['StartDate'], errors='coerce')\n",
    "\n",
    "# Extract only the date portion from the 'StartDate' column\n",
    "df_dates['StartDate'] = df_dates['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Copy a dataframe\n",
    "data = df_dates.copy()\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# Filter out any date after today\n",
    "data = data[data['Start Date'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['Start Date'] >= pd.to_datetime('2015-01-01')]\n",
    "# Rearranged data\n",
    "data = data[['Start Date', 'counts']]\n",
    "# Save data as CSV\n",
    "data.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Featured_Franchisee_data.csv')\n",
    "\n",
    "########################################### F.E. PROGRAMME TYPE ####################################################\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','Franchisee.ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['Franchisee.ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['Franchisee.ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "data_mod = data_mod[data_mod['id'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroup.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstart.csv')\n",
    "\n",
    "##################################################################### Franchisee #############################################################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['Start Date'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "del data['StartDate']\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['Start Date','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['Start Date']\n",
    "# Delete the column we don't need\n",
    "del data_mod['Start Date']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_active = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "# Use the Modality column to check the model performance by Modality\n",
    "data_mod = data[['InactiveDate','ProgrammeType']]\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "# A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "dummy = pd.get_dummies(data_mod['ProgrammeType'])\n",
    "data_mod = data_mod.merge(dummy, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "data_mod['id'] = data_mod['InactiveDate']\n",
    "# Delete the column we don't need\n",
    "del data_mod['InactiveDate']\n",
    "# Reset the index\n",
    "data_mod.set_index('id')\n",
    "# Delete the column we don't need\n",
    "del data_mod['ProgrammeType']\n",
    "# Rearrange the columns from the dataframe\n",
    "data_mod_inactive = data_mod[['id', 'ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']]\n",
    "data_mod_inactive[['ECD Centre', 'Full Week (Daymothers)', 'Playgroup', 'SmartStart ECD']] *= -1\n",
    "\n",
    "data_mod = pd.concat([data_mod_active, data_mod_inactive], axis=0)\n",
    "data_mod = data_mod.sort_values(by='id')\n",
    "\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data_mod = data_mod[data_mod['id'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data_mod = data_mod[data_mod['id'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data_mod['id'] = data_mod['id'].dt.strftime('%Y-%m-%d')\n",
    "# Create a function...\n",
    "def create_time_series(data, column_name):\n",
    "    data = data.sort_values(by='id')\n",
    "    # There is an issue with the code dataframe above.\n",
    "    # It shows the occurence per day, we want accumulated data.\n",
    "    # The cumsum() function computes the values based on the previous one.\n",
    "    data[column_name] = data[column_name].cumsum()\n",
    "    # Selec the columns from the dataset\n",
    "    data = data[['id', column_name]]\n",
    "    # Assign the values for time for FBprophet\n",
    "    data['ds'] = data['id']\n",
    "    # Assign the value variable in FB Prophet\n",
    "    data['y'] = data[column_name]\n",
    "    # Select the needed varibales\n",
    "    data = data[['ds', 'y']]\n",
    "    # Drop rows with null values\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.drop_duplicates(subset=['ds'])\n",
    "    return data\n",
    "\n",
    "data_ecd = create_time_series(data_mod, 'ECD Centre')\n",
    "data_daymother = create_time_series(data_mod, 'Full Week (Daymothers)')\n",
    "data_playgroup = create_time_series(data_mod, 'Playgroup')\n",
    "data_smartstart = create_time_series(data_mod, 'SmartStart ECD')\n",
    "# Save as a dataframe into the base path\n",
    "base_path = 'C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim'\n",
    "# Saving the files as CSV\n",
    "data_ecd.to_csv(base_path + '\\data_ecd_franchisees.csv')\n",
    "data_daymother.to_csv(base_path + '/data_daymother_franchisees.csv')\n",
    "data_playgroup.to_csv(base_path + '/data_playgroupdata_franchisees.csv')\n",
    "data_smartstart.to_csv(base_path + '/data_smartstartdata_franchisees.csv')\n",
    "\n",
    "############################################# F.E. PROVINCES ###########################################\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pytz\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv\")\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "\n",
    "# Filter out any date after today\n",
    "data = data[data['StartDate'] <= pd.to_datetime(date.today())]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Franchisee.Province' ]]\n",
    "df_sd = data[['StartDate', 'Franchisee.Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Franchisee.Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Franchisee.Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Franchisee.Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Franchisee.Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS.csv')\n",
    "\n",
    "############################################################################## FRANCHISEES #################################################\n",
    "\n",
    "# read_csv file:\n",
    "data = pd.read_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project\\Model Predictions/Time-Series-Forecasting/Data/raw/franchisee.csv\")\n",
    "# convert the 'date' column to datetime\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce')\n",
    "# convert the 'date' column to datetime\n",
    "data['InactiveDate'] = pd.to_datetime(data['InactiveDate'], errors='coerce')\n",
    "# Filter out any date after today\n",
    "current_date = datetime.datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "data = data[data['StartDate'] <= current_date]\n",
    "# keep only dates after 2016.01.01\n",
    "data = data[data['StartDate'] >= pd.to_datetime('2015-01-01').tz_localize('UTC')]\n",
    "data['StartDate'] = data['StartDate'].dt.strftime('%Y-%m-%d')\n",
    "# Create a new dataframe to evaluate the performance over time using\n",
    "# Both active and inactive df\n",
    "# Create a dataframe(s)\n",
    "df_id = data[['InactiveDate', 'Province' ]]\n",
    "df_sd = data[['StartDate', 'Province' ]]\n",
    "# Get dummy variables for the two columns\n",
    "dummy_sd = pd.get_dummies(df_sd['Province' ])\n",
    "dummy_id = pd.get_dummies(df_id['Province' ])\n",
    "# Merge the two dataframes\n",
    "df_sd = df_sd.merge(dummy_sd, left_index=True, right_index=True)\n",
    "df_id = df_id.merge(dummy_id, left_index=True, right_index=True)\n",
    "# Change the column name\n",
    "df_sd['id'] = df_sd['StartDate']\n",
    "# Change the column name\n",
    "df_id['id'] = df_id['InactiveDate']\n",
    "# Drop the columns\n",
    "df_sd = df_sd.drop(labels = ['StartDate', 'Province' ], axis =1)\n",
    "df_id = df_id.drop(labels = ['InactiveDate', 'Province' ], axis=1)\n",
    "# Drop the null values\n",
    "df_id = df_id.dropna()\n",
    "# Rearrange the columns\n",
    "df_sd = df_sd[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "df_id = df_id[['id', 'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']]\n",
    "# Assign a negative on the inactive date\n",
    "df_id[['Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal', 'Limpopo',\n",
    "       'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape']] *= -1\n",
    "# Concat the dataframes\n",
    "frames = [df_sd, df_id]\n",
    "df_prov = pd.concat(frames)\n",
    "# Save the dataset\n",
    "df_prov.to_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/interim/Provinces_SS_franchisees.csv')\n",
    "\n",
    "######################################################### DATA VISUALISATION #################################\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "# import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from streamlit import components\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import savgol_filter\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import folium\n",
    "from streamlit_folium import folium_static\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Libraries needed for the tutorial\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Username of your GitHub account\n",
    "\n",
    "username = 'lazola.javu@gmail.com'\n",
    "\n",
    "# Personal Access Token (PAO) from your GitHub account\n",
    "\n",
    "# token = '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'\n",
    "\n",
    "token = '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'\n",
    "\n",
    "########### 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 #################### Token doesn't expire\n",
    "\n",
    "# Creates a re-usable session object with your creds in-built\n",
    "github_session = requests.Session()\n",
    "github_session.auth = (username, token)\n",
    "\n",
    "url_4 = \"https://raw.githubusercontent.com/LazolaJavu/Time-Series-Forecasting/main/Data/external/ECD_Data.csv?token=GHSAT0AAAAAACBIG46DYRXUHQIDWHOEL6B6ZEQEMQA\"\n",
    "ecd_census_data = github_session.get(url_4).content\n",
    "\n",
    "# Reading the downloaded content and making it a pandas dataframe\n",
    "ecd_census_data = pd.read_csv(io.StringIO(ecd_census_data.decode('utf-8')))\n",
    "# Read the shape file\n",
    "shape_file = gpd.read_file(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/external/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\")\n",
    "# Read the shape file\n",
    "gdf = shape_file[shape_file['geonunit'] == 'South Africa']\n",
    "# Read the children data\n",
    "df = pd.read_csv('C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/raw/children_data.csv')\n",
    "# Rename the column table\n",
    "df['Province'] = df['Franchisee.Province']\n",
    "# Group the data by province and count the values\n",
    "province_counts = df['Province'].value_counts().reset_index()\n",
    "province_counts.columns = ['Province', 'Counts']\n",
    "merged_data = gdf.merge(province_counts, left_on='woe_name', right_on='Province', how='left')\n",
    "# Copy the merged data\n",
    "gdf = merged_data.copy()\n",
    "# Save a csv file:\n",
    "gdf.to_csv(\"C:/Users/LazolaJavu/SmartStart/Rabelani Tshidavhu - Child Modelling update_2022/Child Modelling Project/Model Predictions/Time-Series-Forecasting/Data/processed/gdf.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
